{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with Kubernetes on AMD GPUs - Interactive Tutorial\n",
        "\n",
        "**Target Audience**: Infrastructure administrators and DevOps teams exploring AMD GPUs for production Kubernetes workloads\n",
        "\n",
        "This notebook provides hands-on experience with deploying and managing AI inference workloads on Kubernetes clusters with AMD GPUs.\n",
        "\n",
        "## Prerequisites\n",
        "- Kubernetes cluster with AMD GPU support\n",
        "- AMD GPU Operator installed\n",
        "- kubectl configured to access your cluster\n",
        "- vLLM inference service deployed\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Section 1: Environment Setup and Verification\n",
        "\n",
        "Let's start by verifying our Kubernetes cluster and AMD GPU setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import pandas as pd\n",
        "\n",
        "def run_kubectl(command):\n",
        "    \"\"\"Helper function to run kubectl commands and return output\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            f\"kubectl {command}\", \n",
        "            shell=True, \n",
        "            capture_output=True, \n",
        "            text=True, \n",
        "            check=True\n",
        "        )\n",
        "        return result.stdout.strip()\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"Error: {e.stderr.strip()}\"\n",
        "\n",
        "def run_command(command):\n",
        "    \"\"\"Helper function to run any shell command\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(command, shell=True, capture_output=True, text=True, check=True)\n",
        "        return result.stdout.strip()\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"Error: {e.stderr.strip()}\"\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Kubernetes cluster information\n",
        "print(\"üîç Kubernetes Cluster Information\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "cluster_info = run_kubectl(\"cluster-info\")\n",
        "print(cluster_info)\n",
        "\n",
        "print(\"\\nüìä Node Status:\")\n",
        "nodes = run_kubectl(\"get nodes -o wide\")\n",
        "print(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check AMD GPU Operator installation\n",
        "print(\"üéØ AMD GPU Operator Status\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check if AMD GPU Operator namespace exists\n",
        "gpu_ns = run_kubectl(\"get namespace kube-amd-gpu\")\n",
        "print(f\"GPU Operator Namespace: {gpu_ns}\")\n",
        "\n",
        "# Check GPU operator pods\n",
        "print(\"\\nüîß GPU Operator Pods:\")\n",
        "gpu_pods = run_kubectl(\"get pods -n kube-amd-gpu\")\n",
        "print(gpu_pods)\n",
        "\n",
        "# Check node labels for AMD GPUs\n",
        "print(\"\\nüè∑Ô∏è Node GPU Labels:\")\n",
        "node_labels = run_kubectl(\"get nodes -L feature.node.kubernetes.io/amd-gpu\")\n",
        "print(node_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU resources availability\n",
        "print(\"üíæ GPU Resources on Nodes\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "gpu_resources = run_kubectl('get nodes -o custom-columns=NAME:.metadata.name,\"Total GPUs:.status.capacity.amd\\.com/gpu\",\"Allocatable GPUs:.status.allocatable.amd\\.com/gpu\"')\n",
        "print(gpu_resources)\n",
        "\n",
        "# Check for any running GPU workloads\n",
        "print(\"\\nüèÉ Current GPU Workloads:\")\n",
        "gpu_workloads = run_kubectl('get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,\"GPU_REQUESTS:.spec.containers[*].resources.requests.amd\\.com/gpu\"')\n",
        "print(gpu_workloads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Section 2: Deploy and Test vLLM AI Inference\n",
        "\n",
        "Now let's work with AI inference workloads using vLLM on our AMD GPU-enabled Kubernetes cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if vLLM deployment exists\n",
        "print(\"üîç vLLM Deployment Status\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "vllm_deployment = run_kubectl(\"get deployment vllm-inference\")\n",
        "print(f\"Deployment Status: {vllm_deployment}\")\n",
        "\n",
        "# Check vLLM pods\n",
        "print(\"\\nüì¶ vLLM Pods:\")\n",
        "vllm_pods = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "print(vllm_pods)\n",
        "\n",
        "# Check vLLM service\n",
        "print(\"\\nüåê vLLM Service:\")\n",
        "vllm_service = run_kubectl(\"get service vllm-service\")\n",
        "print(vllm_service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get service endpoint for API testing\n",
        "def get_vllm_endpoint():\n",
        "    \"\"\"Get the vLLM service endpoint\"\"\"\n",
        "    try:\n",
        "        # Try to get LoadBalancer external IP\n",
        "        external_ip = run_kubectl(\"get service vllm-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\")\n",
        "        if external_ip and external_ip != \"null\" and \"Error\" not in external_ip:\n",
        "            return f\"http://{external_ip}\"\n",
        "        \n",
        "        # Fallback to NodePort\n",
        "        node_ip = run_kubectl(\"get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}'\")\n",
        "        node_port = run_kubectl(\"get service vllm-service -o jsonpath='{.spec.ports[0].nodePort}'\")\n",
        "        if node_ip and node_port and \"Error\" not in node_ip and \"Error\" not in node_port:\n",
        "            return f\"http://{node_ip}:{node_port}\"\n",
        "        \n",
        "        # Fallback to port-forward (we'll indicate this)\n",
        "        return \"port-forward\"\n",
        "    except:\n",
        "        return \"port-forward\"\n",
        "\n",
        "endpoint = get_vllm_endpoint()\n",
        "print(f\"üåç vLLM Service Endpoint: {endpoint}\")\n",
        "\n",
        "if endpoint == \"port-forward\":\n",
        "    print(\"\\n‚ö†Ô∏è No external access detected. Use port-forward for testing:\")\n",
        "    print(\"   kubectl port-forward service/vllm-service 8000:8000\")\n",
        "    endpoint = \"http://localhost:8000\"\n",
        "    print(f\"   Then use: {endpoint}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test vLLM API health endpoint\n",
        "def test_vllm_health(endpoint_url):\n",
        "    \"\"\"Test vLLM health endpoint\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{endpoint_url}/health\", timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return \"‚úÖ Healthy\", response.text\n",
        "        else:\n",
        "            return f\"‚ùå Status: {response.status_code}\", response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return \"‚ùå Connection Failed\", str(e)\n",
        "\n",
        "print(\"üè• Testing vLLM Health Endpoint\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "if endpoint != \"port-forward\":\n",
        "    status, response = test_vllm_health(endpoint)\n",
        "    print(f\"Health Status: {status}\")\n",
        "    print(f\"Response: {response}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Cannot test health endpoint without port-forward setup.\")\n",
        "    print(\"Run the port-forward command above and then retry this cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test vLLM API with a simple completion request\n",
        "def test_vllm_completion(endpoint_url, prompt, max_tokens=50):\n",
        "    \"\"\"Test vLLM completion endpoint\"\"\"\n",
        "    try:\n",
        "        payload = {\n",
        "            \"model\": \"microsoft/Llama-3.2-1B-Instruct\",\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "        \n",
        "        response = requests.post(\n",
        "            f\"{endpoint_url}/v1/completions\",\n",
        "            json=payload,\n",
        "            headers={\"Content-Type\": \"application/json\"},\n",
        "            timeout=30\n",
        "        )\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            return \"‚úÖ Success\", response.json()\n",
        "        else:\n",
        "            return f\"‚ùå Status: {response.status_code}\", response.text\n",
        "            \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return \"‚ùå Request Failed\", str(e)\n",
        "\n",
        "print(\"üß† Testing vLLM AI Completion\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "test_prompt = \"The benefits of using Kubernetes for AI workloads include:\"\n",
        "\n",
        "if endpoint != \"port-forward\":\n",
        "    print(f\"üìù Prompt: {test_prompt}\")\n",
        "    print(\"\\nüîÑ Generating response...\")\n",
        "    \n",
        "    status, response = test_vllm_completion(endpoint, test_prompt, max_tokens=100)\n",
        "    print(f\"\\nStatus: {status}\")\n",
        "    \n",
        "    if \"Success\" in status:\n",
        "        completion = response['choices'][0]['text']\n",
        "        print(f\"\\nü§ñ AI Response: {completion}\")\n",
        "        print(f\"\\nüìä Usage: {response.get('usage', 'N/A')}\")\n",
        "    else:\n",
        "        print(f\"Error: {response}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Cannot test completion endpoint without port-forward setup.\")\n",
        "    print(\"Run the port-forward command and ensure the service is accessible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Section 3: Scaling and Monitoring GPU Workloads\n",
        "\n",
        "Explore Kubernetes' scaling capabilities with GPU workloads and monitor resource usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check current deployment scale\n",
        "print(\"üìä Current Deployment Scale\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "current_replicas = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.spec.replicas}'\")\n",
        "ready_replicas = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.status.readyReplicas}'\")\n",
        "\n",
        "print(f\"Desired Replicas: {current_replicas}\")\n",
        "print(f\"Ready Replicas: {ready_replicas}\")\n",
        "\n",
        "# Show detailed deployment status\n",
        "print(\"\\nÔøΩÔøΩ Deployment Details:\")\n",
        "deployment_status = run_kubectl(\"describe deployment vllm-inference\")\n",
        "# Show only the relevant parts\n",
        "lines = deployment_status.split('\\n')\n",
        "for line in lines[:15]:  # First 15 lines usually contain the key info\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate scaling the deployment\n",
        "print(\"üöÄ Scaling vLLM Deployment\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Scale to 2 replicas (if we have enough GPUs)\n",
        "print(\"üìà Scaling to 2 replicas...\")\n",
        "scale_result = run_kubectl(\"scale deployment vllm-inference --replicas=2\")\n",
        "print(f\"Scale command result: {scale_result}\")\n",
        "\n",
        "# Wait a moment and check status\n",
        "print(\"\\n‚è≥ Waiting for scaling to take effect...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Check new status\n",
        "new_status = run_kubectl(\"get deployment vllm-inference\")\n",
        "print(f\"\\nüìä Updated Deployment Status:\")\n",
        "print(new_status)\n",
        "\n",
        "# Show pods\n",
        "print(\"\\nüì¶ Pod Status:\")\n",
        "pod_status = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "print(pod_status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor GPU resource usage\n",
        "print(\"üíæ GPU Resource Monitoring\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Check GPU allocation across nodes\n",
        "print(\"üñ•Ô∏è GPU Resources per Node:\")\n",
        "gpu_allocation = run_kubectl('get nodes -o custom-columns=NAME:.metadata.name,\"TOTAL_GPU:.status.capacity.amd\\.com/gpu\",\"ALLOCATABLE_GPU:.status.allocatable.amd\\.com/gpu\"')\n",
        "print(gpu_allocation)\n",
        "\n",
        "# Check which pods are using GPUs\n",
        "print(\"\\nüéØ GPU Usage by Pods:\")\n",
        "gpu_pods = run_kubectl('get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,NODE:.spec.nodeName,\"GPU_REQUEST:.spec.containers[*].resources.requests.amd\\.com/gpu\",\"GPU_LIMIT:.spec.containers[*].resources.limits.amd\\.com/gpu\"')\n",
        "print(gpu_pods)\n",
        "\n",
        "# Show resource usage if available\n",
        "print(\"\\nüìà Node Resource Summary:\")\n",
        "for i in range(3):  # Try to get info for up to 3 nodes\n",
        "    node_info = run_kubectl(f'describe node | grep -A 10 \"Allocated resources\" || echo \"No detailed resource info available\"')\n",
        "    if \"No detailed\" not in node_info:\n",
        "        print(node_info[:500])  # Limit output\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale back to 1 replica for resource efficiency\n",
        "print(\"üìâ Scaling Back to 1 Replica\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "scale_down = run_kubectl(\"scale deployment vllm-inference --replicas=1\")\n",
        "print(f\"Scale down result: {scale_down}\")\n",
        "\n",
        "# Wait and verify\n",
        "time.sleep(5)\n",
        "final_status = run_kubectl(\"get deployment vllm-inference\")\n",
        "print(f\"\\nüìä Final Deployment Status:\")\n",
        "print(final_status)\n",
        "\n",
        "print(\"\\n‚úÖ Scaling demonstration completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Section 4: Advanced Operations and Troubleshooting\n",
        "\n",
        "Learn essential commands for managing GPU workloads in production environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Troubleshooting commands and information gathering\n",
        "print(\"üîç Essential Troubleshooting Commands\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# 1. Check events for any issues\n",
        "print(\"1Ô∏è‚É£ Recent Cluster Events:\")\n",
        "events = run_kubectl(\"get events --sort-by=.metadata.creationTimestamp | tail -10\")\n",
        "print(events)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# 2. Check logs from vLLM pods\n",
        "print(\"2Ô∏è‚É£ vLLM Pod Logs (last 10 lines):\")\n",
        "vllm_logs = run_kubectl(\"logs -l app=vllm-inference --tail=10\")\n",
        "print(vllm_logs)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# 3. Check GPU operator logs\n",
        "print(\"3Ô∏è‚É£ GPU Operator Logs (last 5 lines):\")\n",
        "gpu_operator_logs = run_kubectl(\"logs -n kube-amd-gpu -l app.kubernetes.io/name=gpu-operator-charts --tail=5\")\n",
        "print(gpu_operator_logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance and resource monitoring\n",
        "print(\"üìä Performance Monitoring Commands\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check if metrics are available\n",
        "print(\"1Ô∏è‚É£ Checking GPU Metrics Availability:\")\n",
        "metrics_service = run_kubectl(\"get service -n kube-amd-gpu | grep metrics || echo 'No metrics service found'\")\n",
        "print(metrics_service)\n",
        "\n",
        "# Try to access metrics if available\n",
        "if \"metrics\" in metrics_service and \"No metrics\" not in metrics_service:\n",
        "    print(\"\\n2Ô∏è‚É£ GPU Metrics Endpoint:\")\n",
        "    node_ip = run_kubectl(\"get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\")\n",
        "    print(f\"üìà Metrics available at: http://{node_ip}:32500/metrics\")\n",
        "    print(\"   Use this endpoint with Prometheus for monitoring\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è GPU metrics exporter not found or not configured\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£ Essential Monitoring Commands:\")\n",
        "monitoring_commands = [\n",
        "    \"kubectl top nodes\",\n",
        "    \"kubectl top pods\",\n",
        "    \"kubectl get pods -o wide\",\n",
        "    \"kubectl describe node <node-name>\",\n",
        "    \"kubectl get events --sort-by=.metadata.creationTimestamp\"\n",
        "]\n",
        "\n",
        "for cmd in monitoring_commands:\n",
        "    print(f\"   ‚Ä¢ {cmd}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a summary report\n",
        "print(\"üìã Cluster Summary Report\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Collect all key information\n",
        "summary_data = {\n",
        "    \"Cluster Status\": run_kubectl(\"get nodes --no-headers | wc -l\").strip() + \" nodes\",\n",
        "    \"AMD GPU Nodes\": run_kubectl(\"get nodes -l feature.node.kubernetes.io/amd-gpu=true --no-headers | wc -l\").strip(),\n",
        "    \"GPU Operator Status\": \"Installed\" if \"kube-amd-gpu\" in run_kubectl(\"get namespaces\") else \"Not Installed\",\n",
        "    \"vLLM Deployment\": \"Running\" if \"vllm-inference\" in run_kubectl(\"get deployments\") else \"Not Found\",\n",
        "    \"Total GPU Resources\": run_kubectl('get nodes -o jsonpath=\"{.items[*].status.capacity.amd\\.com/gpu}\"').replace(\" \", \"+\") or \"0\",\n",
        "    \"LoadBalancer Service\": \"Available\" if \"LoadBalancer\" in run_kubectl(\"get service vllm-service\") else \"Not Available\"\n",
        "}\n",
        "\n",
        "# Display summary\n",
        "for key, value in summary_data.items():\n",
        "    print(f\"‚úì {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéâ Tutorial Complete!\")\n",
        "print(\"\\nNext Steps:\")\n",
        "print(\"‚Ä¢ Explore different AI models with vLLM\")\n",
        "print(\"‚Ä¢ Set up monitoring with Prometheus/Grafana\")\n",
        "print(\"‚Ä¢ Implement autoscaling policies\")\n",
        "print(\"‚Ä¢ Configure resource quotas for multi-tenancy\")\n",
        "print(\"‚Ä¢ Explore multi-GPU model parallelism\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Key Takeaways and Next Steps\n",
        "\n",
        "### What You've Learned\n",
        "\n",
        "1. **AMD GPU + Kubernetes Integration**: Successfully deployed the AMD GPU Operator to expose GPU resources as schedulable Kubernetes resources\n",
        "\n",
        "2. **AI Inference Deployment**: Deployed vLLM inference server with proper GPU allocation and external access via LoadBalancer\n",
        "\n",
        "3. **Scaling Operations**: Demonstrated horizontal scaling of GPU workloads and monitoring resource usage\n",
        "\n",
        "### Production Considerations\n",
        "\n",
        "- **Resource Management**: Use resource quotas and limits to prevent GPU resource contention\n",
        "- **Monitoring**: Implement comprehensive monitoring with Prometheus and Grafana\n",
        "- **High Availability**: Deploy across multiple nodes with anti-affinity rules\n",
        "- **Security**: Use network policies and pod security standards\n",
        "\n",
        "### Useful Resources\n",
        "\n",
        "- [AMD GPU Operator Documentation](https://rocm.github.io/gpu-operator/)\n",
        "- [vLLM Documentation](https://docs.vllm.ai/)\n",
        "- [Kubernetes GPU Scheduling](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)\n",
        "- [ROCm Blog Series](https://rocm.blogs.amd.com/artificial-intelligence/k8s-orchestration-part1/README.html)\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** üéâ You now have hands-on experience with AMD GPU-accelerated Kubernetes clusters for AI workloads."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
