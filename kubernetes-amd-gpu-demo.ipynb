{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with Kubernetes on AMD GPUs - Interactive Tutorial\n",
        "\n",
        "**Target Audience**: Infrastructure administrators and DevOps teams exploring AMD GPUs for production Kubernetes workloads\n",
        "\n",
        "This notebook provides hands-on experience with deploying and managing AI inference workloads on Kubernetes clusters with AMD GPUs.\n",
        "\n",
        "## Prerequisites\n",
        "- Ubuntu/Debian server with AMD GPUs\n",
        "- Root/sudo access for system-level operations\n",
        "- At least 2GB RAM and 20GB free disk space\n",
        "- Internet connectivity for package downloads\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Section 0: Kubernetes Prerequisites and Installation\n",
        "\n",
        "Before we can work with AMD GPUs, we need a functioning Kubernetes cluster. This section will check if Kubernetes is installed and guide you through installation if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import pandas as pd\n",
        "\n",
        "def run_command(command, check=False):\n",
        "    \"\"\"Helper function to run shell commands and return output\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            command, \n",
        "            shell=True, \n",
        "            capture_output=True, \n",
        "            text=True, \n",
        "            check=check\n",
        "        )\n",
        "        return result.returncode, result.stdout.strip(), result.stderr.strip()\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return e.returncode, e.stdout.strip() if e.stdout else \"\", e.stderr.strip() if e.stderr else \"\"\n",
        "\n",
        "def run_kubectl(command):\n",
        "    \"\"\"Helper function to run kubectl commands and return output\"\"\"\n",
        "    return run_command(f\"kubectl {command}\")\n",
        "\n",
        "def check_command_exists(command):\n",
        "    \"\"\"Check if a command exists in the system\"\"\"\n",
        "    returncode, _, _ = run_command(f\"which {command}\")\n",
        "    return returncode == 0\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prerequisites check\n",
        "print(\"üîç System Prerequisites Check\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check OS\n",
        "returncode, os_info, _ = run_command(\"cat /etc/os-release | grep PRETTY_NAME\")\n",
        "if returncode == 0:\n",
        "    print(f\"üìü Operating System: {os_info.split('=')[1].strip('\\\"')}\")\n",
        "else:\n",
        "    print(\"‚ùå Could not detect OS\")\n",
        "\n",
        "# Check memory\n",
        "returncode, memory_info, _ = run_command(\"free -h | grep Mem\")\n",
        "if returncode == 0:\n",
        "    memory_total = memory_info.split()[1]\n",
        "    print(f\"üíæ Total Memory: {memory_total}\")\n",
        "    \n",
        "    # Extract numeric value for comparison\n",
        "    memory_gb = float(memory_total.replace('Gi', '').replace('G', '').replace('Mi', '').replace('M', ''))\n",
        "    if 'Mi' in memory_total or 'M' in memory_total:\n",
        "        memory_gb = memory_gb / 1024\n",
        "    \n",
        "    if memory_gb < 2:\n",
        "        print(\"‚ö†Ô∏è Warning: Less than 2GB RAM detected. Kubernetes may not perform well.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Memory check passed\")\n",
        "\n",
        "# Check disk space\n",
        "returncode, disk_info, _ = run_command(\"df -h / | tail -1\")\n",
        "if returncode == 0:\n",
        "    disk_available = disk_info.split()[3]\n",
        "    print(f\"üíø Available Disk Space: {disk_available}\")\n",
        "\n",
        "# Check for AMD GPUs\n",
        "returncode, gpu_info, _ = run_command(\"lspci | grep -i amd\")\n",
        "if returncode == 0 and gpu_info:\n",
        "    print(\"‚úÖ AMD GPUs detected:\")\n",
        "    for line in gpu_info.split('\\n'):\n",
        "        if line.strip():\n",
        "            print(f\"   üéÆ {line.strip()}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No AMD GPUs detected\")\n",
        "\n",
        "# Check sudo access\n",
        "returncode, _, _ = run_command(\"sudo -n true\")\n",
        "if returncode == 0:\n",
        "    print(\"‚úÖ Root/sudo access available\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Root/sudo access may be required for some operations\")\n",
        "\n",
        "print(\"\\n‚úÖ System prerequisites check completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Kubernetes installation status\n",
        "print(\"üîç Kubernetes Installation Check\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check if kubectl is installed\n",
        "kubectl_installed = check_command_exists(\"kubectl\")\n",
        "kubeadm_installed = check_command_exists(\"kubeadm\")\n",
        "kubelet_installed = check_command_exists(\"kubelet\")\n",
        "\n",
        "print(f\"kubectl installed: {'‚úÖ' if kubectl_installed else '‚ùå'}\")\n",
        "print(f\"kubeadm installed: {'‚úÖ' if kubeadm_installed else '‚ùå'}\")\n",
        "print(f\"kubelet installed: {'‚úÖ' if kubelet_installed else '‚ùå'}\")\n",
        "\n",
        "# If kubectl is installed, check cluster connectivity\n",
        "cluster_accessible = False\n",
        "if kubectl_installed:\n",
        "    returncode, version_output, error = run_kubectl(\"version --client\")\n",
        "    if returncode == 0:\n",
        "        print(f\"\\nüìã kubectl version: {version_output.split()[2] if len(version_output.split()) > 2 else 'Unknown'}\")\n",
        "        \n",
        "        # Check cluster access\n",
        "        returncode, cluster_info, error = run_kubectl(\"cluster-info\")\n",
        "        if returncode == 0:\n",
        "            print(\"‚úÖ Kubernetes cluster is accessible\")\n",
        "            cluster_accessible = True\n",
        "            \n",
        "            # Show basic cluster info\n",
        "            print(\"\\nüìä Cluster Information:\")\n",
        "            for line in cluster_info.split('\\n')[:3]:  # First 3 lines\n",
        "                if line.strip():\n",
        "                    print(f\"   {line.strip()}\")\n",
        "        else:\n",
        "            print(\"‚ùå Kubernetes cluster not accessible\")\n",
        "            print(f\"   Error: {error}\")\n",
        "\n",
        "# Determine what needs to be done\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "if not kubectl_installed or not kubeadm_installed or not kubelet_installed:\n",
        "    print(\"üö® KUBERNETES INSTALLATION REQUIRED\")\n",
        "    print(\"\\nYou need to install Kubernetes. Options:\")\n",
        "    print(\"1. Run the installation script: sudo ./install-kubernetes.sh\")\n",
        "    print(\"2. Or execute the installation cells below\")\n",
        "elif not cluster_accessible:\n",
        "    print(\"üö® KUBERNETES CLUSTER SETUP REQUIRED\")\n",
        "    print(\"\\nKubernetes is installed but cluster is not accessible.\")\n",
        "    print(\"You may need to initialize the cluster or fix configuration.\")\n",
        "else:\n",
        "    print(\"‚úÖ KUBERNETES IS READY\")\n",
        "    print(\"\\nYou can proceed to the AMD GPU Operator installation section.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üõ†Ô∏è Kubernetes Installation (Run only if needed)\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT**: Run these cells only if Kubernetes is not installed or not accessible. This requires root/sudo privileges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Run the installation script (Recommended)\n",
        "print(\"üöÄ Option 1: Automated Kubernetes Installation\")\n",
        "print(\"=\" * 45)\n",
        "print(\"\\nTo install Kubernetes automatically, run this command in a terminal:\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"sudo ./install-kubernetes.sh\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nThis script will:\")\n",
        "print(\"‚Ä¢ Install containerd container runtime\")\n",
        "print(\"‚Ä¢ Install Kubernetes components (kubelet, kubeadm, kubectl)\")\n",
        "print(\"‚Ä¢ Initialize the cluster\")\n",
        "print(\"‚Ä¢ Install Calico CNI networking\")\n",
        "print(\"‚Ä¢ Configure single-node cluster\")\n",
        "print(\"‚Ä¢ Verify installation\")\n",
        "print(\"\\n‚è±Ô∏è Estimated time: 10-15 minutes\")\n",
        "print(\"\\nüí° After running the script, restart this notebook and re-run the checks above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 2: Step-by-step installation (Advanced users)\n",
        "# WARNING: This cell demonstrates the installation steps but requires careful execution\n",
        "\n",
        "print(\"üîß Option 2: Manual Step-by-Step Installation\")\n",
        "print(\"=\" * 45)\n",
        "print(\"\\n‚ö†Ô∏è WARNING: This is for demonstration purposes.\")\n",
        "print(\"For actual installation, use Option 1 (install-kubernetes.sh script)\")\n",
        "print(\"\\nKey installation steps that the script performs:\")\n",
        "\n",
        "installation_steps = [\n",
        "    \"1. Disable swap: swapoff -a\",\n",
        "    \"2. Load kernel modules: modprobe overlay && modprobe br_netfilter\",\n",
        "    \"3. Configure sysctl: net.bridge.bridge-nf-call-iptables = 1\",\n",
        "    \"4. Install containerd container runtime\",\n",
        "    \"5. Add Kubernetes repositories\",\n",
        "    \"6. Install kubelet, kubeadm, kubectl\",\n",
        "    \"7. Initialize cluster: kubeadm init\",\n",
        "    \"8. Configure kubectl access\",\n",
        "    \"9. Install Calico CNI\",\n",
        "    \"10. Remove control-plane taints for single-node setup\"\n",
        "]\n",
        "\n",
        "for step in installation_steps:\n",
        "    print(f\"   {step}\")\n",
        "\n",
        "print(\"\\nüîó For detailed commands, see the install-kubernetes.sh script\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Kubernetes installation after manual setup\n",
        "print(\"‚úÖ Post-Installation Verification\")\n",
        "print(\"=\" * 35)\n",
        "print(\"\\nRun this cell after Kubernetes installation to verify:\")\n",
        "\n",
        "# Re-check Kubernetes\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    print(\"‚úÖ kubectl is now available\")\n",
        "    \n",
        "    # Check cluster access\n",
        "    returncode, output, error = run_kubectl(\"get nodes\")\n",
        "    if returncode == 0:\n",
        "        print(\"‚úÖ Cluster is accessible\")\n",
        "        print(\"\\nüìä Node Status:\")\n",
        "        print(output)\n",
        "        \n",
        "        # Check system pods\n",
        "        returncode, pods_output, _ = run_kubectl(\"get pods -n kube-system\")\n",
        "        if returncode == 0:\n",
        "            print(\"\\nüîß System Pods Status:\")\n",
        "            print(pods_output)\n",
        "    else:\n",
        "        print(f\"‚ùå Cluster access failed: {error}\")\nelse:\n",
        "    print(\"‚ùå kubectl still not available\")\n",
        "\n",
        "print(\"\\nüéØ Ready to proceed to AMD GPU Operator installation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Section 1: Environment Setup and Verification\n",
        "\n",
        "Now that we have Kubernetes running, let's verify our cluster and check for AMD GPU setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Kubernetes cluster information (updated for better error handling)\n",
        "print(\"üîç Kubernetes Cluster Information\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not found. Please install Kubernetes first.\")\n",
        "    print(\"   Run: sudo ./install-kubernetes.sh\")\n",
        "else:\n",
        "    returncode, cluster_info, error = run_kubectl(\"cluster-info\")\n",
        "    if returncode == 0:\n",
        "        print(cluster_info)\n",
        "        \n",
        "        print(\"\\nüìä Node Status:\")\n",
        "        returncode, nodes, _ = run_kubectl(\"get nodes -o wide\")\n",
        "        if returncode == 0:\n",
        "            print(nodes)\n",
        "        else:\n",
        "            print(\"‚ùå Could not get node information\")\n",
        "    else:\n",
        "        print(f\"‚ùå Cluster not accessible: {error}\")\n",
        "        print(\"\\nüí° Troubleshooting tips:\")\n",
        "        print(\"   ‚Ä¢ Check if kubelet is running: systemctl status kubelet\")\n",
        "        print(\"   ‚Ä¢ Verify KUBECONFIG: echo $KUBECONFIG\")\n",
        "        print(\"   ‚Ä¢ Try: export KUBECONFIG=/etc/kubernetes/admin.conf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check AMD GPU Operator installation\n",
        "print(\"üéØ AMD GPU Operator Status\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot check GPU Operator\")\n",
        "else:\n",
        "    # Check if AMD GPU Operator namespace exists\n",
        "    returncode, gpu_ns, _ = run_kubectl(\"get namespace kube-amd-gpu\")\n",
        "    if returncode == 0:\n",
        "        print(\"‚úÖ AMD GPU Operator namespace found\")\n",
        "        \n",
        "        # Check GPU operator pods\n",
        "        print(\"\\nüîß GPU Operator Pods:\")\n",
        "        returncode, gpu_pods, _ = run_kubectl(\"get pods -n kube-amd-gpu\")\n",
        "        if returncode == 0:\n",
        "            print(gpu_pods)\n",
        "        else:\n",
        "            print(\"‚ùå Could not get GPU operator pods\")\n",
        "        \n",
        "        # Check node labels for AMD GPUs\n",
        "        print(\"\\nüè∑Ô∏è Node GPU Labels:\")\n",
        "        returncode, node_labels, _ = run_kubectl(\"get nodes -L feature.node.kubernetes.io/amd-gpu\")\n",
        "        if returncode == 0:\n",
        "            print(node_labels)\n",
        "        else:\n",
        "            print(\"‚ùå Could not get node GPU labels\")\n",
        "    else:\n",
        "        print(\"‚ùå AMD GPU Operator not installed\")\n",
        "        print(\"\\nüí° To install AMD GPU Operator:\")\n",
        "        print(\"   Run: ./install-amd-gpu-operator.sh\")\n",
        "        print(\"   Or proceed to the GPU Operator installation section below\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU resources availability\n",
        "print(\"üíæ GPU Resources on Nodes\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot check GPU resources\")\n",
        "else:\n",
        "    returncode, gpu_resources, _ = run_kubectl('get nodes -o custom-columns=NAME:.metadata.name,\"Total GPUs:.status.capacity.amd\\.com/gpu\",\"Allocatable GPUs:.status.allocatable.amd\\.com/gpu\"')\n",
        "    if returncode == 0:\n",
        "        print(gpu_resources)\n",
        "        \n",
        "        # Check for any running GPU workloads\n",
        "        print(\"\\nüèÉ Current GPU Workloads:\")\n",
        "        returncode, gpu_workloads, _ = run_kubectl('get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,\"GPU_REQUESTS:.spec.containers[*].resources.requests.amd\\.com/gpu\"')\n",
        "        if returncode == 0:\n",
        "            # Filter only pods that actually request GPUs\n",
        "            lines = gpu_workloads.split('\\n')\n",
        "            header = lines[0]\n",
        "            gpu_pods = [line for line in lines[1:] if line.split()[-1] not in ['<none>', ''] and len(line.split()) > 2]\n",
        "            \n",
        "            if gpu_pods:\n",
        "                print(header)\n",
        "                for pod in gpu_pods:\n",
        "                    print(pod)\n",
        "            else:\n",
        "                print(\"   No GPU workloads currently running\")\n",
        "        else:\n",
        "            print(\"‚ùå Could not check GPU workloads\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not check GPU resources\")\n",
        "        print(\"   This is normal if AMD GPU Operator is not installed yet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Section 2: Deploy and Test vLLM AI Inference\n",
        "\n",
        "Now let's work with AI inference workloads using vLLM on our AMD GPU-enabled Kubernetes cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if vLLM deployment exists\n",
        "print(\"üîç vLLM Deployment Status\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot check vLLM deployment\")\n",
        "else:\n",
        "    returncode, vllm_deployment, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "    if returncode == 0:\n",
        "        print(f\"‚úÖ vLLM Deployment found:\")\n",
        "        print(vllm_deployment)\n",
        "        \n",
        "        # Check vLLM pods\n",
        "        print(\"\\nüì¶ vLLM Pods:\")\n",
        "        returncode, vllm_pods, _ = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "        if returncode == 0:\n",
        "            print(vllm_pods)\n",
        "        else:\n",
        "            print(\"‚ùå Could not get vLLM pods\")\n",
        "        \n",
        "        # Check vLLM service\n",
        "        print(\"\\nüåê vLLM Service:\")\n",
        "        returncode, vllm_service, _ = run_kubectl(\"get service vllm-service\")\n",
        "        if returncode == 0:\n",
        "            print(vllm_service)\n",
        "        else:\n",
        "            print(\"‚ùå Could not get vLLM service\")\n",
        "    else:\n",
        "        print(\"‚ùå vLLM deployment not found\")\n",
        "        print(\"\\nüí° To deploy vLLM inference:\")\n",
        "        print(\"   1. First ensure AMD GPU Operator is installed\")\n",
        "        print(\"   2. Run: ./deploy-vllm-inference.sh\")\n",
        "        print(\"   3. Or continue with the deployment sections below\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get service endpoint for API testing\n",
        "def get_vllm_endpoint():\n",
        "    \"\"\"Get the vLLM service endpoint\"\"\"\n",
        "    if not check_command_exists(\"kubectl\"):\n",
        "        return \"kubectl-not-available\"\n",
        "        \n",
        "    try:\n",
        "        # Try to get LoadBalancer external IP\n",
        "        returncode, external_ip, _ = run_kubectl(\"get service vllm-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\")\n",
        "        if returncode == 0 and external_ip and external_ip != \"null\" and external_ip.strip():\n",
        "            return f\"http://{external_ip.strip()}\"\n",
        "        \n",
        "        # Fallback to NodePort\n",
        "        returncode, node_ip, _ = run_kubectl(\"get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\")\n",
        "        returncode2, node_port, _ = run_kubectl(\"get service vllm-service -o jsonpath='{.spec.ports[0].nodePort}'\")\n",
        "        \n",
        "        if returncode == 0 and returncode2 == 0 and node_ip and node_port:\n",
        "            return f\"http://{node_ip.strip()}:{node_port.strip()}\"\n",
        "        \n",
        "        # Fallback to port-forward indication\n",
        "        return \"port-forward\"\n",
        "    except:\n",
        "        return \"port-forward\"\n",
        "\n",
        "endpoint = get_vllm_endpoint()\n",
        "print(f\"üåç vLLM Service Endpoint Detection\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if endpoint == \"kubectl-not-available\":\n",
        "    print(\"‚ùå kubectl not available - cannot detect vLLM endpoint\")\n",
        "elif endpoint == \"port-forward\":\n",
        "    print(\"‚ö†Ô∏è No external access detected. Use port-forward for testing:\")\n",
        "    print(\"\\nüí° To access vLLM service:\")\n",
        "    print(\"   kubectl port-forward service/vllm-service 8000:8000\")\n",
        "    print(\"   Then use: http://localhost:8000\")\n",
        "    endpoint = \"http://localhost:8000\"\n",
        "else:\n",
        "    print(f\"‚úÖ vLLM Service accessible at: {endpoint}\")\n",
        "    print(f\"   API endpoint: {endpoint}/v1/completions\")\n",
        "    print(f\"   Health check: {endpoint}/health\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test vLLM API health endpoint\n",
        "def test_vllm_health(endpoint_url):\n",
        "    \"\"\"Test vLLM health endpoint\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{endpoint_url}/health\", timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return \"‚úÖ Healthy\", response.text\n",
        "        else:\n",
        "            return f\"‚ùå Status: {response.status_code}\", response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return \"‚ùå Connection Failed\", str(e)\n",
        "\n",
        "print(\"üè• Testing vLLM Health Endpoint\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "if endpoint in [\"kubectl-not-available\", \"port-forward\"]:\n",
        "    print(\"‚ö†Ô∏è Cannot test health endpoint without proper service access.\")\n",
        "    if endpoint == \"port-forward\":\n",
        "        print(\"\\nüí° To test the health endpoint:\")\n",
        "        print(\"   1. Run: kubectl port-forward service/vllm-service 8000:8000\")\n",
        "        print(\"   2. In another terminal: curl http://localhost:8000/health\")\n",
        "        print(\"   3. Or re-run this cell after setting up port-forward\")\nelse:\n",
        "    status, response = test_vllm_health(endpoint)\n",
        "    print(f\"Health Status: {status}\")\n",
        "    print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test vLLM API with a simple completion request\n",
        "def test_vllm_completion(endpoint_url, prompt, max_tokens=50):\n",
        "    \"\"\"Test vLLM completion endpoint\"\"\"\n",
        "    try:\n",
        "        payload = {\n",
        "            \"model\": \"microsoft/Llama-3.2-1B-Instruct\",\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "        \n",
        "        response = requests.post(\n",
        "            f\"{endpoint_url}/v1/completions\",\n",
        "            json=payload,\n",
        "            headers={\"Content-Type\": \"application/json\"},\n",
        "            timeout=30\n",
        "        )\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            return \"‚úÖ Success\", response.json()\n",
        "        else:\n",
        "            return f\"‚ùå Status: {response.status_code}\", response.text\n",
        "            \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return \"‚ùå Request Failed\", str(e)\n",
        "\n",
        "print(\"üß† Testing vLLM AI Completion\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "test_prompt = \"The benefits of using Kubernetes for AI workloads include:\"\n",
        "\n",
        "if endpoint in [\"kubectl-not-available\", \"port-forward\"]:\n",
        "    print(\"‚ö†Ô∏è Cannot test completion endpoint without proper service access.\")\n",
        "    if endpoint == \"port-forward\":\n",
        "        print(f\"\\nüìù Test prompt: {test_prompt}\")\n",
        "        print(\"\\nüí° To test AI completion:\")\n",
        "        print(\"   1. Run: kubectl port-forward service/vllm-service 8000:8000\")\n",
        "        print(\"   2. Use curl or re-run this cell after port-forward setup\")\n",
        "        print(\"\\nüîß Example curl command:\")\n",
        "        print('   curl -X POST http://localhost:8000/v1/completions \\\\')\n",
        "        print('     -H \"Content-Type: application/json\" \\\\')\n",
        "        print('     -d \\'{\"model\": \"microsoft/Llama-3.2-1B-Instruct\", \"prompt\": \"' + test_prompt + '\", \"max_tokens\": 100}\\'')\nelse:\n",
        "    print(f\"üìù Prompt: {test_prompt}\")\n",
        "    print(\"\\nüîÑ Generating response...\")\n",
        "    \n",
        "    status, response = test_vllm_completion(endpoint, test_prompt, max_tokens=100)\n",
        "    print(f\"\\nStatus: {status}\")\n",
        "    \n",
        "    if \"Success\" in status:\n",
        "        completion = response['choices'][0]['text']\n",
        "        print(f\"\\nü§ñ AI Response: {completion}\")\n",
        "        print(f\"\\nüìä Usage: {response.get('usage', 'N/A')}\")\n",
        "    else:\n",
        "        print(f\"Error: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Section 3: Scaling and Monitoring GPU Workloads\n",
        "\n",
        "Explore Kubernetes' scaling capabilities with GPU workloads and monitor resource usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check current deployment scale\n",
        "print(\"üìä Current Deployment Scale\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot check deployment scale\")\n",
        "else:\n",
        "    returncode, current_replicas, _ = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.spec.replicas}'\")\n",
        "    returncode2, ready_replicas, _ = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.status.readyReplicas}'\")\n",
        "    \n",
        "    if returncode == 0:\n",
        "        print(f\"Desired Replicas: {current_replicas}\")\n",
        "        print(f\"Ready Replicas: {ready_replicas if returncode2 == 0 else 'Unknown'}\")\n",
        "        \n",
        "        # Show detailed deployment status\n",
        "        print(\"\\nüìã Deployment Details:\")\n",
        "        returncode, deployment_status, _ = run_kubectl(\"describe deployment vllm-inference\")\n",
        "        if returncode == 0:\n",
        "            # Show only the relevant parts\n",
        "            lines = deployment_status.split('\\n')\n",
        "            for line in lines[:15]:  # First 15 lines usually contain the key info\n",
        "                if line.strip():\n",
        "                    print(line)\n",
        "        else:\n",
        "            print(\"‚ùå Could not get deployment details\")\n",
        "    else:\n",
        "        print(\"‚ùå vLLM deployment not found\")\n",
        "        print(\"   Deploy vLLM first using: ./deploy-vllm-inference.sh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate scaling the deployment\n",
        "print(\"üöÄ Scaling vLLM Deployment\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot scale deployment\")\n",
        "else:\n",
        "    # Check if deployment exists first\n",
        "    returncode, _, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "    if returncode != 0:\n",
        "        print(\"‚ùå vLLM deployment not found - cannot scale\")\n",
        "        print(\"   Deploy vLLM first using: ./deploy-vllm-inference.sh\")\n",
        "    else:\n",
        "        # Scale to 2 replicas (if we have enough GPUs)\n",
        "        print(\"üìà Scaling to 2 replicas...\")\n",
        "        returncode, scale_result, error = run_kubectl(\"scale deployment vllm-inference --replicas=2\")\n",
        "        if returncode == 0:\n",
        "            print(f\"‚úÖ Scale command executed: {scale_result}\")\n",
        "            \n",
        "            # Wait a moment and check status\n",
        "            print(\"\\n‚è≥ Waiting for scaling to take effect...\")\n",
        "            time.sleep(10)\n",
        "            \n",
        "            # Check new status\n",
        "            returncode, new_status, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "            if returncode == 0:\n",
        "                print(f\"\\nüìä Updated Deployment Status:\")\n",
        "                print(new_status)\n",
        "            \n",
        "            # Show pods\n",
        "            print(\"\\nüì¶ Pod Status:\")\n",
        "            returncode, pod_status, _ = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "            if returncode == 0:\n",
        "                print(pod_status)\n",
        "            else:\n",
        "                print(\"‚ùå Could not get pod status\")\n",
        "        else:\n",
        "            print(f\"‚ùå Scale command failed: {error}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor GPU resource usage\n",
        "print(\"üíæ GPU Resource Monitoring\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot monitor GPU resources\")\n",
        "else:\n",
        "    # Check GPU allocation across nodes\n",
        "    print(\"üñ•Ô∏è GPU Resources per Node:\")\n",
        "    returncode, gpu_allocation, _ = run_kubectl('get nodes -o custom-columns=NAME:.metadata.name,\"TOTAL_GPU:.status.capacity.amd\\.com/gpu\",\"ALLOCATABLE_GPU:.status.allocatable.amd\\.com/gpu\"')\n",
        "    if returncode == 0:\n",
        "        print(gpu_allocation)\n",
        "    else:\n",
        "        print(\"‚ùå Could not get GPU allocation info\")\n",
        "        print(\"   This is normal if AMD GPU Operator is not installed\")\n",
        "    \n",
        "    # Check which pods are using GPUs\n",
        "    print(\"\\nüéØ GPU Usage by Pods:\")\n",
        "    returncode, gpu_pods, _ = run_kubectl('get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,NODE:.spec.nodeName,\"GPU_REQUEST:.spec.containers[*].resources.requests.amd\\.com/gpu\",\"GPU_LIMIT:.spec.containers[*].resources.limits.amd\\.com/gpu\"')\n",
        "    if returncode == 0:\n",
        "        # Filter only pods that actually request GPUs\n",
        "        lines = gpu_pods.split('\\n')\n",
        "        header = lines[0]\n",
        "        gpu_requesting_pods = [line for line in lines[1:] if line and not line.endswith('<none>') and len(line.split()) >= 4 and line.split()[-2] not in ['<none>', '']]\n",
        "        \n",
        "        if gpu_requesting_pods:\n",
        "            print(header)\n",
        "            for pod in gpu_requesting_pods:\n",
        "                print(pod)\n",
        "        else:\n",
        "            print(\"   No pods currently requesting GPUs\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not check GPU usage by pods\")\n",
        "    \n",
        "    # Show resource usage summary\n",
        "    print(\"\\nüìà Node Resource Summary:\")\n",
        "    returncode, node_info, _ = run_kubectl('describe nodes | grep -A 5 \"Allocated resources\" | head -10')\n",
        "    if returncode == 0 and node_info.strip():\n",
        "        print(node_info[:500])  # Limit output\n",
        "    else:\n",
        "        print(\"   Detailed resource info not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale back to 1 replica for resource efficiency\n",
        "print(\"üìâ Scaling Back to 1 Replica\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot scale deployment\")\n",
        "else:\n",
        "    # Check if deployment exists\n",
        "    returncode, _, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "    if returncode != 0:\n",
        "        print(\"‚ùå vLLM deployment not found\")\n",
        "    else:\n",
        "        returncode, scale_down, error = run_kubectl(\"scale deployment vllm-inference --replicas=1\")\n",
        "        if returncode == 0:\n",
        "            print(f\"‚úÖ Scale down executed: {scale_down}\")\n",
        "            \n",
        "            # Wait and verify\n",
        "            time.sleep(5)\n",
        "            returncode, final_status, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "            if returncode == 0:\n",
        "                print(f\"\\nüìä Final Deployment Status:\")\n",
        "                print(final_status)\n",
        "            \n",
        "            print(\"\\n‚úÖ Scaling demonstration completed!\")\n",
        "        else:\n",
        "            print(f\"‚ùå Scale down failed: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Section 4: Advanced Operations and Troubleshooting\n",
        "\n",
        "Learn essential commands for managing GPU workloads in production environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Troubleshooting commands and information gathering\n",
        "print(\"üîç Essential Troubleshooting Commands\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot run troubleshooting commands\")\n",
        "    print(\"\\nüí° Install Kubernetes first: sudo ./install-kubernetes.sh\")\n",
        "else:\n",
        "    # 1. Check events for any issues\n",
        "    print(\"1Ô∏è‚É£ Recent Cluster Events:\")\n",
        "    returncode, events, _ = run_kubectl(\"get events --sort-by=.metadata.creationTimestamp | tail -10\")\n",
        "    if returncode == 0:\n",
        "        print(events)\n",
        "    else:\n",
        "        print(\"‚ùå Could not get cluster events\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # 2. Check logs from vLLM pods\n",
        "    print(\"2Ô∏è‚É£ vLLM Pod Logs (last 10 lines):\")\n",
        "    returncode, vllm_logs, _ = run_kubectl(\"logs -l app=vllm-inference --tail=10\")\n",
        "    if returncode == 0:\n",
        "        if vllm_logs.strip():\n",
        "            print(vllm_logs)\n",
        "        else:\n",
        "            print(\"   No vLLM pods found or no logs available\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not get vLLM logs (deployment may not exist)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # 3. Check GPU operator logs\n",
        "    print(\"3Ô∏è‚É£ GPU Operator Logs (last 5 lines):\")\n",
        "    returncode, gpu_operator_logs, _ = run_kubectl(\"logs -n kube-amd-gpu -l app.kubernetes.io/name=gpu-operator-charts --tail=5\")\n",
        "    if returncode == 0:\n",
        "        if gpu_operator_logs.strip():\n",
        "            print(gpu_operator_logs)\n",
        "        else:\n",
        "            print(\"   No GPU operator pods found\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not get GPU operator logs (may not be installed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance and resource monitoring\n",
        "print(\"üìä Performance Monitoring Commands\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot run monitoring commands\")\n",
        "else:\n",
        "    # Check if metrics are available\n",
        "    print(\"1Ô∏è‚É£ Checking GPU Metrics Availability:\")\n",
        "    returncode, metrics_service, _ = run_kubectl(\"get service -n kube-amd-gpu\")\n",
        "    if returncode == 0:\n",
        "        # Look for metrics service\n",
        "        if \"metrics\" in metrics_service.lower():\n",
        "            print(\"‚úÖ GPU metrics service found:\")\n",
        "            for line in metrics_service.split('\\n'):\n",
        "                if 'metrics' in line.lower():\n",
        "                    print(f\"   {line}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No metrics service found in kube-amd-gpu namespace\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not check services (GPU operator may not be installed)\")\n",
        "    \n",
        "    # Try to access metrics if available\n",
        "    if returncode == 0 and \"metrics\" in metrics_service.lower():\n",
        "        print(\"\\n2Ô∏è‚É£ GPU Metrics Endpoint:\")\n",
        "        returncode, node_ip, _ = run_kubectl(\"get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\")\n",
        "        if returncode == 0 and node_ip:\n",
        "            print(f\"üìà Metrics available at: http://{node_ip.strip()}:32500/metrics\")\n",
        "            print(\"   Use this endpoint with Prometheus for monitoring\")\n",
        "        else:\n",
        "            print(\"‚ùå Could not determine node IP for metrics access\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è GPU metrics exporter not found or not configured\")\n",
        "    \n",
        "    print(\"\\n3Ô∏è‚É£ Essential Monitoring Commands:\")\n",
        "    monitoring_commands = [\n",
        "        \"kubectl top nodes\",\n",
        "        \"kubectl top pods\",\n",
        "        \"kubectl get pods -o wide\",\n",
        "        \"kubectl describe node <node-name>\",\n",
        "        \"kubectl get events --sort-by=.metadata.creationTimestamp\"\n",
        "    ]\n",
        "    \n",
        "    for cmd in monitoring_commands:\n",
        "        print(f\"   ‚Ä¢ {cmd}\")\n",
        "    \n",
        "    # Try to run kubectl top nodes if metrics-server is available\n",
        "    print(\"\\n4Ô∏è‚É£ Current Resource Usage (if metrics-server available):\")\n",
        "    returncode, top_output, _ = run_kubectl(\"top nodes\")\n",
        "    if returncode == 0:\n",
        "        print(top_output)\n",
        "    else:\n",
        "        print(\"   Metrics-server not available (normal for basic installations)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a summary report\n",
        "print(\"üìã Cluster Summary Report\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "if not check_command_exists(\"kubectl\"):\n",
        "    print(\"‚ùå kubectl not available - cannot generate cluster summary\")\n",
        "    summary_data = {\n",
        "        \"Kubernetes Status\": \"Not Installed\",\n",
        "        \"kubectl Available\": \"No\",\n",
        "        \"Recommendation\": \"Run: sudo ./install-kubernetes.sh\"\n",
        "    }\n",
        "else:\n",
        "    # Collect all key information\n",
        "    summary_data = {}\n",
        "    \n",
        "    # Cluster nodes\n",
        "    returncode, nodes, _ = run_kubectl(\"get nodes --no-headers\")\n",
        "    if returncode == 0:\n",
        "        node_count = len([line for line in nodes.split('\\n') if line.strip()])\n",
        "        summary_data[\"Cluster Status\"] = f\"{node_count} node(s)\"\n",
        "    else:\n",
        "        summary_data[\"Cluster Status\"] = \"Not accessible\"\n",
        "    \n",
        "    # AMD GPU nodes\n",
        "    returncode, gpu_nodes, _ = run_kubectl(\"get nodes -l feature.node.kubernetes.io/amd-gpu=true --no-headers\")\n",
        "    if returncode == 0:\n",
        "        gpu_node_count = len([line for line in gpu_nodes.split('\\n') if line.strip()])\n",
        "        summary_data[\"AMD GPU Nodes\"] = str(gpu_node_count)\n",
        "    else:\n",
        "        summary_data[\"AMD GPU Nodes\"] = \"Unknown\"\n",
        "    \n",
        "    # GPU Operator status\n",
        "    returncode, _, _ = run_kubectl(\"get namespace kube-amd-gpu\")\n",
        "    summary_data[\"GPU Operator Status\"] = \"Installed\" if returncode == 0 else \"Not Installed\"\n",
        "    \n",
        "    # vLLM deployment\n",
        "    returncode, _, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "    summary_data[\"vLLM Deployment\"] = \"Running\" if returncode == 0 else \"Not Found\"\n",
        "    \n",
        "    # Total GPU resources\n",
        "    returncode, gpu_capacity, _ = run_kubectl('get nodes -o jsonpath=\"{.items[*].status.capacity.amd\\.com/gpu}\"')\n",
        "    if returncode == 0 and gpu_capacity.strip():\n",
        "        # Sum up GPU resources\n",
        "        try:\n",
        "            gpus = [int(x) for x in gpu_capacity.split() if x.isdigit()]\n",
        "            total_gpus = sum(gpus) if gpus else 0\n",
        "            summary_data[\"Total GPU Resources\"] = str(total_gpus)\n",
        "        except:\n",
        "            summary_data[\"Total GPU Resources\"] = \"0\"\n",
        "    else:\n",
        "        summary_data[\"Total GPU Resources\"] = \"0\"\n",
        "    \n",
        "    # LoadBalancer service\n",
        "    returncode, service_info, _ = run_kubectl(\"get service vllm-service\")\n",
        "    if returncode == 0:\n",
        "        summary_data[\"LoadBalancer Service\"] = \"Available\" if \"LoadBalancer\" in service_info else \"Not LoadBalancer\"\n",
        "    else:\n",
        "        summary_data[\"LoadBalancer Service\"] = \"Not Available\"\n",
        "\n",
        "# Display summary\n",
        "for key, value in summary_data.items():\n",
        "    print(f\"‚úì {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    print(\"üéâ Tutorial Complete!\")\n",
        "    print(\"\\nNext Steps:\")\n",
        "    print(\"‚Ä¢ Explore different AI models with vLLM\")\n",
        "    print(\"‚Ä¢ Set up monitoring with Prometheus/Grafana\")\n",
        "    print(\"‚Ä¢ Implement autoscaling policies\")\n",
        "    print(\"‚Ä¢ Configure resource quotas for multi-tenancy\")\n",
        "    print(\"‚Ä¢ Explore multi-GPU model parallelism\")\nelse:\n",
        "    print(\"üö® Setup Required!\")\n",
        "    print(\"\\nNext Steps:\")\n",
        "    print(\"‚Ä¢ Install Kubernetes: sudo ./install-kubernetes.sh\")\n",
        "    print(\"‚Ä¢ Install AMD GPU Operator: ./install-amd-gpu-operator.sh\")\n",
        "    print(\"‚Ä¢ Deploy vLLM: ./deploy-vllm-inference.sh\")\n",
        "    print(\"‚Ä¢ Re-run this notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Key Takeaways and Next Steps\n",
        "\n",
        "### What You've Learned\n",
        "\n",
        "1. **Complete Infrastructure Setup**: From bare Ubuntu server to production GPU-accelerated Kubernetes cluster\n",
        "\n",
        "2. **AMD GPU + Kubernetes Integration**: Successfully deployed the AMD GPU Operator to expose GPU resources as schedulable Kubernetes resources\n",
        "\n",
        "3. **AI Inference Deployment**: Deployed vLLM inference server with proper GPU allocation and external access via LoadBalancer\n",
        "\n",
        "4. **Scaling Operations**: Demonstrated horizontal scaling of GPU workloads and monitoring resource usage\n",
        "\n",
        "### Production Considerations\n",
        "\n",
        "- **Resource Management**: Use resource quotas and limits to prevent GPU resource contention\n",
        "- **Monitoring**: Implement comprehensive monitoring with Prometheus and Grafana\n",
        "- **High Availability**: Deploy across multiple nodes with anti-affinity rules\n",
        "- **Security**: Use network policies and pod security standards\n",
        "- **Backup & Recovery**: Implement proper backup strategies for persistent data\n",
        "\n",
        "### Installation Summary\n",
        "\n",
        "**Complete Stack Installation Commands:**\n",
        "```bash\n",
        "# Step 0: Install Kubernetes (if needed)\n",
        "sudo ./install-kubernetes.sh\n",
        "\n",
        "# Step 1: Install AMD GPU Operator\n",
        "./install-amd-gpu-operator.sh\n",
        "\n",
        "# Step 2: Deploy AI Inference Workload\n",
        "./deploy-vllm-inference.sh\n",
        "```\n",
        "\n",
        "### Useful Resources\n",
        "\n",
        "- [AMD GPU Operator Documentation](https://rocm.github.io/gpu-operator/)\n",
        "- [vLLM Documentation](https://docs.vllm.ai/)\n",
        "- [Kubernetes GPU Scheduling](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)\n",
        "- [ROCm Blog Series](https://rocm.blogs.amd.com/artificial-intelligence/k8s-orchestration-part1/README.html)\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** üéâ You now have hands-on experience with the complete stack: from bare metal Ubuntu to production AMD GPU-accelerated Kubernetes clusters running AI inference workloads."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
