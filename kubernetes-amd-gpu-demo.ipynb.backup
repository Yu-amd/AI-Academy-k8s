{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with Kubernetes on AMD GPUs - Interactive Tutorial\n",
        "\n",
        "**Target Audience**: Infrastructure administrators and DevOps teams exploring AMD GPUs for production Kubernetes workloads\n",
        "\n",
        "This notebook provides hands-on experience with deploying and managing AI inference workloads on Kubernetes clusters with AMD GPUs.\n",
        "\n",
        "## Prerequisites\n",
        "- Ubuntu/Debian server with AMD GPUs\n",
        "- Root/sudo access for system-level operations\n",
        "- At least 2GB RAM and 20GB free disk space\n",
        "- Internet connectivity for package downloads\n",
        "\n",
        "## \ud83c\udfaf Tutorial Approach\n",
        "This notebook is **completely self-contained**. You can execute all installation and deployment steps directly within Jupyter cells - no need to switch to terminal!\n",
        "\n",
        "## \ud83d\udea8 IMPORTANT: Getting Started\n",
        "**Before running any other cells:**\n",
        "1. **\ud83d\udcd6 Read through the entire notebook first** to understand the process\n",
        "2. **\u26a1 Run the \"Ensure Scripts Are Executable\" cell** (Step 0 below) to fix file permissions\n",
        "3. **\ud83d\udd0d Run the \"Debugging\" cell** if you encounter any issues\n",
        "4. **\ud83d\udccb Follow the numbered sections in order**\n",
        "\n",
        "This tutorial will take you from a fresh system to a production-ready GPU-accelerated AI platform in about 30-45 minutes.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Section 0: System Prerequisites Check\n",
        "\n",
        "Let's start by checking system requirements and current status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import pandas as pd\n",
        "\n",
        "def run_command(command, check=False, show_output=True):\n",
        "    \"\"\"Helper function to run shell commands and return output\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            command, \n",
        "            shell=True, \n",
        "            capture_output=True, \n",
        "            text=True, \n",
        "            check=check\n",
        "        )\n",
        "        if show_output and result.stdout:\n",
        "            print(result.stdout)\n",
        "        if show_output and result.stderr and result.returncode != 0:\n",
        "            print(f\"Error: {result.stderr}\")\n",
        "        return result.returncode, result.stdout.strip(), result.stderr.strip()\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        if show_output:\n",
        "            print(f\"Command failed: {e}\")\n",
        "        return e.returncode, e.stdout.strip() if e.stdout else \"\", e.stderr.strip() if e.stderr else \"\"\n",
        "\n",
        "def run_kubectl(command):\n",
        "    \"\"\"Helper function to run kubectl commands and return output\"\"\"\n",
        "    return run_command(f\"kubectl {command}\", show_output=False)\n",
        "\n",
        "def check_command_exists(command):\n",
        "    \"\"\"Check if a command exists in the system\"\"\"\n",
        "    returncode, _, _ = run_command(f\"which {command}\", show_output=False)\n",
        "    return returncode == 0\n",
        "\n",
        "print(\"\u2705 Helper functions loaded\")\n",
        "print(\"\ud83c\udfaf Ready to execute installation scripts directly in the notebook!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u26a1 Step 0: Ensure Scripts Are Executable\n",
        "\n",
        "**\ud83d\udea8 IMPORTANT: Run this cell first!**\n",
        "\n",
        "This cell ensures all installation scripts have the correct executable permissions. File uploads to cloud platforms often lose executable permissions, so we fix that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure all scripts are executable (run this first!)\n",
        "print(\"\ud83d\udd27 Ensuring Scripts Are Executable\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "import os\n",
        "import stat\n",
        "\n",
        "# List of scripts that need to be executable\n",
        "scripts = [\n",
        "    'install-kubernetes.sh',\n",
        "    'install-amd-gpu-operator.sh',\n",
        "    'deploy-vllm-inference.sh'\n",
        "]\n",
        "\n",
        "print(\"\ud83d\udcc1 Current directory:\", os.getcwd())\n",
        "print(\"\\n\ud83d\udd27 Making scripts executable:\")\n",
        "\n",
        "for script in scripts:\n",
        "    if os.path.exists(script):\n",
        "        try:\n",
        "            # Get current permissions\n",
        "            current_perms = oct(os.stat(script).st_mode)[-3:]\n",
        "            \n",
        "            # Make executable (755 permissions)\n",
        "            os.chmod(script, stat.S_IRWXU | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n",
        "            \n",
        "            # Verify it worked\n",
        "            new_perms = oct(os.stat(script).st_mode)[-3:]\n",
        "            is_executable = os.access(script, os.X_OK)\n",
        "            \n",
        "            print(f\"   \u2705 {script}\")\n",
        "            print(f\"      Old permissions: {current_perms}\")\n",
        "            print(f\"      New permissions: {new_perms}\")\n",
        "            print(f\"      Executable: {'\u2705' if is_executable else '\u274c'}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   \u274c {script} - Error: {e}\")\n",
        "    else:\n",
        "        print(f\"   \u26a0\ufe0f {script} - File not found\")\n",
        "        print(f\"      Expected location: {os.path.join(os.getcwd(), script)}\")\n",
        "\n",
        "# Verify all scripts using ls -la\n",
        "print(\"\\n\ud83d\udccb Final verification:\")\n",
        "returncode, output, error = run_command(\"ls -la *.sh 2>/dev/null || echo 'No shell scripts found'\", show_output=False)\n",
        "if returncode == 0:\n",
        "    print(\"   Script permissions:\")\n",
        "    for line in output.split('\\n'):\n",
        "        if line.strip() and '.sh' in line:\n",
        "            # Parse ls -la output to show key info\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 9:\n",
        "                perms = parts[0]\n",
        "                filename = parts[-1]\n",
        "                executable_status = \"\u2705\" if perms[3] == 'x' else \"\u274c\"\n",
        "                print(f\"     {perms} {filename} {executable_status}\")\nelse:\n",
        "    print(f\"   \u274c Could not verify: {error}\")\n",
        "\n",
        "# Test script execution\n",
        "print(\"\\n\ud83e\uddea Testing script execution:\")\n",
        "test_script = 'install-kubernetes.sh'\n",
        "if os.path.exists(test_script):\n",
        "    # Test with help flag to avoid actual execution\n",
        "    returncode, output, error = run_command(f\"./{test_script} --help 2>&1 | head -3 || echo 'Script responds to execution'\", show_output=False)\n",
        "    \n",
        "    if 'permission denied' in output.lower() or 'permission denied' in error.lower():\n",
        "        print(f\"   \u274c Permission denied - script not properly executable\")\n",
        "    elif 'command not found' in output.lower() or 'command not found' in error.lower():\n",
        "        print(f\"   \u274c Command not found - check script location\")\n",
        "    else:\n",
        "        print(f\"   \u2705 Script can be executed successfully\")\nelse:\n",
        "    print(f\"   \u26a0\ufe0f Test script {test_script} not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\ud83c\udfaf Scripts are now ready for execution!\")\n",
        "print(\"\ud83d\udca1 If you still have issues, the debug cell below can help troubleshoot.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Check current working directory and script availability\n",
        "print(\"\ud83d\udd0d Debugging Script Location and Permissions\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "import os\n",
        "\n",
        "# Check current working directory\n",
        "current_dir = os.getcwd()\n",
        "print(f\"\ud83d\udcc1 Current working directory: {current_dir}\")\n",
        "\n",
        "# List files in current directory\n",
        "print(f\"\\n\ud83d\udccb Files in current directory:\")\n",
        "try:\n",
        "    files = os.listdir('.')\n",
        "    for file in sorted(files):\n",
        "        if os.path.isfile(file):\n",
        "            size = os.path.getsize(file)\n",
        "            permissions = oct(os.stat(file).st_mode)[-3:]\n",
        "            executable = \"\u2705\" if os.access(file, os.X_OK) else \"\u274c\"\n",
        "            print(f\"   {file} ({size} bytes, {permissions}, executable: {executable})\")\n",
        "        elif os.path.isdir(file):\n",
        "            print(f\"   \ud83d\udcc1 {file}/\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error listing files: {e}\")\n",
        "\n",
        "# Specifically check for our scripts\n",
        "scripts_to_check = [\n",
        "    'install-kubernetes.sh',\n",
        "    'install-amd-gpu-operator.sh', \n",
        "    'deploy-vllm-inference.sh'\n",
        "]\n",
        "\n",
        "print(f\"\\n\ud83d\udd0d Checking for required scripts:\")\n",
        "for script in scripts_to_check:\n",
        "    if os.path.exists(script):\n",
        "        is_executable = os.access(script, os.X_OK)\n",
        "        size = os.path.getsize(script)\n",
        "        print(f\"   \u2705 {script} (exists, {size} bytes, executable: {'\u2705' if is_executable else '\u274c'})\")\n",
        "        \n",
        "        # Check if we can read the first few lines\n",
        "        try:\n",
        "            with open(script, 'r') as f:\n",
        "                first_line = f.readline().strip()\n",
        "                print(f\"      First line: {first_line}\")\n",
        "        except:\n",
        "            print(f\"      \u26a0\ufe0f Cannot read file\")\n",
        "    else:\n",
        "        print(f\"   \u274c {script} (not found)\")\n",
        "\n",
        "# Test command execution from notebook\n",
        "print(f\"\\n\ud83e\uddea Testing command execution:\")\n",
        "returncode, output, error = run_command(\"pwd\", show_output=False)\n",
        "print(f\"   pwd command: {output}\")\n",
        "\n",
        "returncode, output, error = run_command(\"whoami\", show_output=False)\n",
        "print(f\"   Current user: {output}\")\n",
        "\n",
        "returncode, output, error = run_command(\"ls -la *.sh 2>/dev/null || echo 'No .sh files found'\", show_output=False)\n",
        "print(f\"   Shell scripts found: {output}\")\n",
        "\n",
        "# Check if we can execute scripts with full path\n",
        "print(f\"\\n\ud83d\udd27 Testing script execution:\")\n",
        "full_path_script = os.path.join(current_dir, 'install-kubernetes.sh')\n",
        "returncode, output, error = run_command(f\"ls -la {full_path_script}\", show_output=False)\n",
        "print(f\"   Full path check: {output if output else error}\")\n",
        "\n",
        "if os.path.exists('install-kubernetes.sh'):\n",
        "    # Test if we can execute with full path\n",
        "    returncode, output, error = run_command(f\"{full_path_script} --help 2>/dev/null || echo 'Script cannot be executed'\", show_output=False)\n",
        "    print(f\"   Script execution test: {output[:100] if output else error[:100]}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Troubleshooting tips:\")\n",
        "print(f\"   \u2022 If scripts are not executable: run 'chmod +x *.sh' in a terminal\")\n",
        "print(f\"   \u2022 If scripts are missing: ensure you're in the correct directory\")\n",
        "print(f\"   \u2022 Current directory should contain all .sh scripts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd27 Fix Script Permissions (Run if needed)\n",
        "\n",
        "If the debug cell above shows that scripts are not executable or not found, run this cell to fix permissions and verify location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix script permissions and verify location\n",
        "print(\"\ud83d\udd27 Fixing Script Permissions and Location\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "import os\n",
        "\n",
        "# Change to the correct directory (same as notebook)\n",
        "notebook_dir = os.path.dirname(os.path.abspath('__file__')) if '__file__' in globals() else os.getcwd()\n",
        "print(f\"\ud83d\udcc1 Ensuring we're in notebook directory: {notebook_dir}\")\n",
        "os.chdir(notebook_dir)\n",
        "\n",
        "# Make scripts executable\n",
        "scripts = ['install-kubernetes.sh', 'install-amd-gpu-operator.sh', 'deploy-vllm-inference.sh']\n",
        "\n",
        "print(f\"\\n\ud83d\udd27 Making scripts executable:\")\n",
        "for script in scripts:\n",
        "    if os.path.exists(script):\n",
        "        try:\n",
        "            # Make executable\n",
        "            os.chmod(script, 0o755)\n",
        "            print(f\"   \u2705 {script} - permissions fixed\")\n",
        "        except Exception as e:\n",
        "            print(f\"   \u274c {script} - failed to fix permissions: {e}\")\n",
        "    else:\n",
        "        print(f\"   \u274c {script} - file not found\")\n",
        "\n",
        "# Verify the fix worked\n",
        "print(f\"\\n\u2705 Verification:\")\n",
        "returncode, output, error = run_command(\"ls -la *.sh\", show_output=False)\n",
        "if returncode == 0:\n",
        "    print(f\"   Script permissions:\")\n",
        "    for line in output.split('\\n'):\n",
        "        if line.strip():\n",
        "            print(f\"     {line}\")\n",
        "else:\n",
        "    print(f\"   \u274c Could not list scripts: {error}\")\n",
        "\n",
        "# Test execution\n",
        "if os.path.exists('install-kubernetes.sh'):\n",
        "    print(f\"\\n\ud83e\uddea Testing script execution:\")\n",
        "    returncode, output, error = run_command(\"./install-kubernetes.sh --help 2>&1 | head -3\", show_output=False)\n",
        "    if 'command not found' not in output.lower() and 'permission denied' not in output.lower():\n",
        "        print(f\"   \u2705 Script can be executed\")\n",
        "    else:\n",
        "        print(f\"   \u274c Script execution issue: {output[:100]}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 If issues persist, try running the following in a terminal:\")\n",
        "print(f\"   cd {os.getcwd()}\")\n",
        "print(f\"   chmod +x *.sh\")\n",
        "print(f\"   ls -la *.sh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prerequisites check\n",
        "print(\"\ud83d\udd0d System Prerequisites Check\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check OS\n",
        "returncode, os_info, _ = run_command(\"cat /etc/os-release | grep PRETTY_NAME\", show_output=False)\n",
        "if returncode == 0:\n",
        "    print(f\"\ud83d\udcdf Operating System: {os_info.split('=')[1].strip('\\\"')}\")\n",
        "else:\n",
        "    print(\"\u274c Could not detect OS\")\n",
        "\n",
        "# Check memory\n",
        "returncode, memory_info, _ = run_command(\"free -h | grep Mem\", show_output=False)\n",
        "if returncode == 0:\n",
        "    memory_total = memory_info.split()[1]\n",
        "    print(f\"\ud83d\udcbe Total Memory: {memory_total}\")\n",
        "    \n",
        "    # Extract numeric value for comparison\n",
        "    memory_gb = float(memory_total.replace('Gi', '').replace('G', '').replace('Mi', '').replace('M', ''))\n",
        "    if 'Mi' in memory_total or 'M' in memory_total:\n",
        "        memory_gb = memory_gb / 1024\n",
        "    \n",
        "    if memory_gb < 2:\n",
        "        print(\"\u26a0\ufe0f Warning: Less than 2GB RAM detected. Kubernetes may not perform well.\")\n",
        "    else:\n",
        "        print(\"\u2705 Memory check passed\")\n",
        "\n",
        "# Check disk space\n",
        "returncode, disk_info, _ = run_command(\"df -h / | tail -1\", show_output=False)\n",
        "if returncode == 0:\n",
        "    disk_available = disk_info.split()[3]\n",
        "    print(f\"\ud83d\udcbf Available Disk Space: {disk_available}\")\n",
        "\n",
        "# Check for AMD GPUs\n",
        "returncode, gpu_info, _ = run_command(\"lspci | grep -i amd\", show_output=False)\n",
        "if returncode == 0 and gpu_info:\n",
        "    print(\"\u2705 AMD GPUs detected:\")\n",
        "    for line in gpu_info.split('\\n'):\n",
        "        if line.strip():\n",
        "            print(f\"   \ud83c\udfae {line.strip()}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f No AMD GPUs detected\")\n",
        "\n",
        "# Check sudo access\n",
        "returncode, _, _ = run_command(\"sudo -n true\", show_output=False)\n",
        "if returncode == 0:\n",
        "    print(\"\u2705 Root/sudo access available\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Root/sudo access may be required for some operations\")\n",
        "\n",
        "print(\"\\n\u2705 System prerequisites check completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Kubernetes installation status\n",
        "print(\"\ud83d\udd0d Kubernetes Installation Status\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check if Kubernetes components are installed\n",
        "kubectl_installed = check_command_exists(\"kubectl\")\n",
        "kubeadm_installed = check_command_exists(\"kubeadm\")\n",
        "kubelet_installed = check_command_exists(\"kubelet\")\n",
        "\n",
        "print(f\"kubectl installed: {'\u2705' if kubectl_installed else '\u274c'}\")\n",
        "print(f\"kubeadm installed: {'\u2705' if kubeadm_installed else '\u274c'}\")\n",
        "print(f\"kubelet installed: {'\u2705' if kubelet_installed else '\u274c'}\")\n",
        "\n",
        "# If kubectl is installed, check cluster connectivity\n",
        "cluster_accessible = False\n",
        "if kubectl_installed:\n",
        "    returncode, version_output, error = run_kubectl(\"version --client\")\n",
        "    if returncode == 0:\n",
        "        print(f\"\\n\ud83d\udccb kubectl version: {version_output.split()[2] if len(version_output.split()) > 2 else 'Unknown'}\")\n",
        "        \n",
        "        # Check cluster access\n",
        "        returncode, cluster_info, error = run_kubectl(\"cluster-info\")\n",
        "        if returncode == 0:\n",
        "            print(\"\u2705 Kubernetes cluster is accessible\")\n",
        "            cluster_accessible = True\n",
        "            \n",
        "            # Show basic cluster info\n",
        "            print(\"\\n\ud83d\udcca Cluster Information:\")\n",
        "            for line in cluster_info.split('\\n')[:3]:  # First 3 lines\n",
        "                if line.strip():\n",
        "                    print(f\"   {line.strip()}\")\n",
        "        else:\n",
        "            print(\"\u274c Kubernetes cluster not accessible\")\n",
        "            print(f\"   Error: {error}\")\n",
        "\n",
        "# Store status for next cells\n",
        "kubernetes_needs_installation = not kubectl_installed or not kubeadm_installed or not kubelet_installed\n",
        "kubernetes_needs_cluster_setup = kubectl_installed and not cluster_accessible\n",
        "\n",
        "# Determine what needs to be done\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "if kubernetes_needs_installation:\n",
        "    print(\"\ud83d\udea8 KUBERNETES INSTALLATION REQUIRED\")\n",
        "    print(\"\\n\ud83d\udca1 Execute the next cell to install Kubernetes automatically!\")\n",
        "elif kubernetes_needs_cluster_setup:\n",
        "    print(\"\ud83d\udea8 KUBERNETES CLUSTER SETUP REQUIRED\")\n",
        "    print(\"\\n\ud83d\udca1 Execute the next cell to initialize the Kubernetes cluster!\")\n",
        "else:\n",
        "    print(\"\u2705 KUBERNETES IS READY\")\n",
        "    print(\"\\n\ud83c\udfaf You can skip to the AMD GPU Operator installation section.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Section 1: One-Click Kubernetes Installation\n",
        "\n",
        "**Execute this cell only if Kubernetes installation is required** (as indicated by the check above).\n",
        "\n",
        "This cell will execute the complete Kubernetes installation script directly in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-click Kubernetes installation with improved error handling\n",
        "# NOTE: This script handles common cloud platform issues like missing overlay modules\n",
        "# Only run this if Kubernetes is not installed\n",
        "\n",
        "print(\"\ud83d\ude80 Kubernetes Installation\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "if kubernetes_needs_installation or kubernetes_needs_cluster_setup:\n",
        "    print(\"\ud83d\udce6 Starting Kubernetes installation...\")\n",
        "    print(\"\u23f1\ufe0f This will take 10-15 minutes. Please be patient.\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    # Ensure we're in the right directory and script exists\n",
        "    import os\n",
        "    current_dir = os.getcwd()\n",
        "    script_path = os.path.join(current_dir, 'install-kubernetes.sh')\n",
        "    \n",
        "    if not os.path.exists(script_path):\n",
        "        print(f\"\u274c Script not found at: {script_path}\")\n",
        "        print(f\"\ud83d\udcc1 Current directory: {current_dir}\")\n",
        "        print(f\"\ud83d\udccb Available files: {os.listdir(current_dir)}\")\n",
        "        print(\"\\n\ud83d\udca1 Please ensure you're running this notebook from the correct directory.\")\n",
        "    else:\n",
        "        # Make sure script is executable\n",
        "        os.chmod(script_path, 0o755)\n",
        "        \n",
        "        # Execute with full path and better error handling\n",
        "        print(f\"\ud83d\udd27 Executing: sudo {script_path}\")\n",
        "        returncode, output, error = run_command(f\"sudo {script_path}\", show_output=True)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        if returncode == 0:\n",
        "            print(\"\u2705 Kubernetes installation completed successfully!\")\n",
        "            print(\"\\n\ud83d\udd04 Re-checking Kubernetes status...\")\n",
        "            \n",
        "            # Re-check status\n",
        "            time.sleep(5)\n",
        "            returncode, nodes, _ = run_kubectl(\"get nodes\")\n",
        "            if returncode == 0:\n",
        "                print(\"\\n\ud83d\udcca Cluster Nodes:\")\n",
        "                print(nodes)\n",
        "            else:\n",
        "                print(\"\u26a0\ufe0f Cluster may still be initializing. Wait a moment and check manually.\")\n",
        "        else:\n",
        "            print(f\"\u274c Kubernetes installation failed with return code: {returncode}\")\n",
        "            print(\"\\n\ud83d\udd0d Common issues and solutions:\")\n",
        "            print(\"   \u2022 Permission denied: Ensure you have sudo access\")\n",
        "            print(\"   \u2022 Script not found: Run the debug/fix cells above\")\n",
        "            print(\"   \u2022 Network issues: Check internet connectivity\")\n",
        "            print(\"   \u2022 Insufficient resources: Ensure 2GB+ RAM and 20GB+ disk\")\n",
        "            if error:\n",
        "                print(f\"\\n\ud83d\udcdd Error details: {error[:200]}...\")\nelse:\n",
        "    print(\"\u2705 Kubernetes is already installed and accessible.\")\n",
        "    print(\"\\n\ud83c\udfaf Proceeding to next section...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udee0\ufe0f Common Installation Issues and Solutions\n",
        "\n",
        "If the Kubernetes installation encounters issues, here are common problems and solutions:\n",
        "\n",
        "#### \ud83d\udd27 Kernel Module Issues\n",
        "**Error**: `modprobe: FATAL: Module overlay not found`\n",
        "- **Cause**: Some cloud instances don't have overlay kernel modules\n",
        "- **Solution**: The script automatically handles this with fallbacks\n",
        "- **Status**: \u2705 Already fixed in the installation script\n",
        "\n",
        "#### \ud83d\udcbe Memory Issues\n",
        "**Error**: Installation fails due to insufficient memory\n",
        "- **Cause**: Less than 2GB RAM available\n",
        "- **Solution**: Use a larger instance or add swap space\n",
        "\n",
        "#### \ud83c\udf10 Network Issues\n",
        "**Error**: Cannot download packages\n",
        "- **Cause**: Internet connectivity or firewall issues\n",
        "- **Solution**: Check internet access and security group settings\n",
        "\n",
        "#### \ud83d\udd10 Permission Issues\n",
        "**Error**: Permission denied during installation\n",
        "- **Cause**: Insufficient sudo privileges\n",
        "- **Solution**: Ensure you have sudo access or run as root\n",
        "\n",
        "If issues persist, check the error messages above and refer to the troubleshooting section at the end of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfae Section 2: One-Click AMD GPU Operator Installation\n",
        "\n",
        "Now let's install the AMD GPU Operator to enable GPU support in our Kubernetes cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if AMD GPU Operator is already installed\n",
        "print(\"\ud83c\udfaf AMD GPU Operator Installation Check\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "gpu_operator_installed = False\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    returncode, _, _ = run_kubectl(\"get namespace kube-amd-gpu\")\n",
        "    if returncode == 0:\n",
        "        print(\"\u2705 AMD GPU Operator namespace found\")\n",
        "        \n",
        "        # Check GPU operator pods\n",
        "        returncode, gpu_pods, _ = run_kubectl(\"get pods -n kube-amd-gpu\")\n",
        "        if returncode == 0:\n",
        "            print(\"\\n\ud83d\udd27 GPU Operator Pods:\")\n",
        "            print(gpu_pods)\n",
        "            gpu_operator_installed = True\n",
        "        else:\n",
        "            print(\"\u26a0\ufe0f GPU Operator namespace exists but pods not found\")\n",
        "    else:\n",
        "        print(\"\u274c AMD GPU Operator not installed\")\nelse:\n",
        "    print(\"\u274c kubectl not available - install Kubernetes first\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if gpu_operator_installed:\n",
        "    print(\"\u2705 AMD GPU OPERATOR IS ALREADY INSTALLED\")\n",
        "    print(\"\\n\ud83c\udfaf You can skip to the vLLM deployment section.\")\nelse:\n",
        "    print(\"\ud83d\udea8 AMD GPU OPERATOR INSTALLATION REQUIRED\")\n",
        "    print(\"\\n\ud83d\udca1 Execute the next cell to install AMD GPU Operator!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-click AMD GPU Operator installation with improved error handling\n",
        "print(\"\ud83c\udfae AMD GPU Operator Installation\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if not gpu_operator_installed:\n",
        "    print(\"\ud83d\udce6 Starting AMD GPU Operator installation...\")\n",
        "    print(\"\u23f1\ufe0f This will take 5-10 minutes.\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    # Ensure script exists and is executable\n",
        "    import os\n",
        "    script_path = os.path.join(os.getcwd(), 'install-amd-gpu-operator.sh')\n",
        "    \n",
        "    if not os.path.exists(script_path):\n",
        "        print(f\"\u274c Script not found at: {script_path}\")\n",
        "        print(\"\ud83d\udca1 Please run the debug/fix cells above.\")\n",
        "    else:\n",
        "        os.chmod(script_path, 0o755)\n",
        "        \n",
        "        # Execute the AMD GPU Operator installation script\n",
        "        print(f\"\ud83d\udd27 Executing: {script_path}\")\n",
        "        returncode, output, error = run_command(script_path, show_output=True)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        if returncode == 0:\n",
        "            print(\"\u2705 AMD GPU Operator installation completed successfully!\")\n",
        "            \n",
        "            # Check GPU resources\n",
        "            print(\"\\n\ud83d\udd04 Checking GPU resources...\")\n",
        "            time.sleep(10)\n",
        "            returncode, gpu_resources, _ = run_kubectl('get nodes -o custom-columns=NAME:.metadata.name,\"Total GPUs:.status.capacity.amd\\.com/gpu\",\"Allocatable GPUs:.status.allocatable.amd\\.com/gpu\"')\n",
        "            if returncode == 0:\n",
        "                print(\"\\n\ud83d\udcbe GPU Resources:\")\n",
        "                print(gpu_resources)\n",
        "            else:\n",
        "                print(\"\u26a0\ufe0f GPU resources not yet visible. They may take a few minutes to appear.\")\n",
        "        else:\n",
        "            print(f\"\u274c AMD GPU Operator installation failed with return code: {returncode}\")\n",
        "            print(\"\\n\ud83d\udca1 Check the error messages above and retry if needed.\")\nelse:\n",
        "    print(\"\u2705 AMD GPU Operator is already installed.\")\n",
        "    print(\"\\n\ud83c\udfaf Proceeding to next section...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\udd16 Section 3: One-Click vLLM AI Inference Deployment\n",
        "\n",
        "Now let's deploy a production-ready AI inference service using vLLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if vLLM is already deployed\n",
        "print(\"\ud83e\udd16 vLLM Deployment Status Check\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "vllm_deployed = False\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    returncode, _, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "    if returncode == 0:\n",
        "        print(\"\u2705 vLLM deployment found\")\n",
        "        \n",
        "        # Check deployment status\n",
        "        returncode, deployment_status, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "        if returncode == 0:\n",
        "            print(\"\\n\ud83d\udcca Deployment Status:\")\n",
        "            print(deployment_status)\n",
        "            vllm_deployed = True\n",
        "        \n",
        "        # Check service\n",
        "        returncode, service_status, _ = run_kubectl(\"get service vllm-service\")\n",
        "        if returncode == 0:\n",
        "            print(\"\\n\ud83c\udf10 Service Status:\")\n",
        "            print(service_status)\n",
        "    else:\n",
        "        print(\"\u274c vLLM deployment not found\")\nelse:\n",
        "    print(\"\u274c kubectl not available\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if vllm_deployed:\n",
        "    print(\"\u2705 vLLM IS ALREADY DEPLOYED\")\n",
        "    print(\"\\n\ud83c\udfaf You can proceed to testing the API.\")\nelse:\n",
        "    print(\"\ud83d\udea8 vLLM DEPLOYMENT REQUIRED\")\n",
        "    print(\"\\n\ud83d\udca1 Execute the next cell to deploy vLLM!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-click vLLM deployment with improved error handling\n",
        "print(\"\ud83e\udd16 vLLM AI Inference Deployment\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "if not vllm_deployed:\n",
        "    print(\"\ud83d\udce6 Starting vLLM deployment...\")\n",
        "    print(\"\u23f1\ufe0f This will take 5-10 minutes (includes downloading the model).\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    # Ensure script exists and is executable\n",
        "    import os\n",
        "    script_path = os.path.join(os.getcwd(), 'deploy-vllm-inference.sh')\n",
        "    \n",
        "    if not os.path.exists(script_path):\n",
        "        print(f\"\u274c Script not found at: {script_path}\")\n",
        "        print(\"\ud83d\udca1 Please run the debug/fix cells above.\")\n",
        "    else:\n",
        "        os.chmod(script_path, 0o755)\n",
        "        \n",
        "        # Execute the vLLM deployment script\n",
        "        print(f\"\ud83d\udd27 Executing: {script_path}\")\n",
        "        returncode, output, error = run_command(script_path, show_output=True)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        if returncode == 0:\n",
        "            print(\"\u2705 vLLM deployment completed successfully!\")\n",
        "            \n",
        "            # Check deployment status\n",
        "            print(\"\\n\ud83d\udd04 Checking deployment status...\")\n",
        "            time.sleep(5)\n",
        "            \n",
        "            returncode, pods, _ = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "            if returncode == 0:\n",
        "                print(\"\\n\ud83d\udce6 vLLM Pods:\")\n",
        "                print(pods)\n",
        "            \n",
        "            returncode, service, _ = run_kubectl(\"get service vllm-service\")\n",
        "            if returncode == 0:\n",
        "                print(\"\\n\ud83c\udf10 vLLM Service:\")\n",
        "                print(service)\n",
        "        else:\n",
        "            print(f\"\u274c vLLM deployment failed with return code: {returncode}\")\n",
        "            print(\"\\n\ud83d\udca1 Check the error messages above and retry if needed.\")\nelse:\n",
        "    print(\"\u2705 vLLM is already deployed.\")\n",
        "    print(\"\\n\ud83c\udfaf Proceeding to API testing...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\uddea Section 4: Test AI Inference API\n",
        "\n",
        "Let's test our deployed AI inference service!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get service endpoint for API testing\n",
        "def get_vllm_endpoint():\n",
        "    \"\"\"Get the vLLM service endpoint\"\"\"\n",
        "    if not check_command_exists(\"kubectl\"):\n",
        "        return \"kubectl-not-available\"\n",
        "        \n",
        "    try:\n",
        "        # Try to get LoadBalancer external IP\n",
        "        returncode, external_ip, _ = run_kubectl(\"get service vllm-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\")\n",
        "        if returncode == 0 and external_ip and external_ip != \"null\" and external_ip.strip():\n",
        "            return f\"http://{external_ip.strip()}\"\n",
        "        \n",
        "        # Fallback to NodePort\n",
        "        returncode, node_ip, _ = run_kubectl(\"get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\")\n",
        "        returncode2, node_port, _ = run_kubectl(\"get service vllm-service -o jsonpath='{.spec.ports[0].nodePort}'\")\n",
        "        \n",
        "        if returncode == 0 and returncode2 == 0 and node_ip and node_port:\n",
        "            return f\"http://{node_ip.strip()}:{node_port.strip()}\"\n",
        "        \n",
        "        # Last resort: port-forward indication\n",
        "        return \"port-forward\"\n",
        "    except:\n",
        "        return \"port-forward\"\n",
        "\n",
        "print(\"\ud83c\udf0d vLLM Service Endpoint Detection\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "endpoint = get_vllm_endpoint()\n",
        "\n",
        "if endpoint == \"kubectl-not-available\":\n",
        "    print(\"\u274c kubectl not available - cannot detect vLLM endpoint\")\nelif endpoint == \"port-forward\":\n",
        "    print(\"\u26a0\ufe0f External access not available. Setting up port-forward...\")\n",
        "    print(\"\\n\ud83d\udd27 Creating port-forward tunnel...\")\n",
        "    \n",
        "    # Start port-forward in background\n",
        "    import threading\n",
        "    import subprocess\n",
        "    \n",
        "    def port_forward():\n",
        "        subprocess.run([\"kubectl\", \"port-forward\", \"service/vllm-service\", \"8000:8000\"])\n",
        "    \n",
        "    # Start port-forward in a separate thread\n",
        "    pf_thread = threading.Thread(target=port_forward, daemon=True)\n",
        "    pf_thread.start()\n",
        "    time.sleep(5)  # Give port-forward time to establish\n",
        "    \n",
        "    endpoint = \"http://localhost:8000\"\n",
        "    print(f\"\u2705 Port-forward established. Using: {endpoint}\")\nelse:\n",
        "    print(f\"\u2705 vLLM Service accessible at: {endpoint}\")\n",
        "    print(f\"   API endpoint: {endpoint}/v1/completions\")\n",
        "    print(f\"   Health check: {endpoint}/health\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test vLLM API health endpoint\n",
        "def test_vllm_health(endpoint_url):\n",
        "    \"\"\"Test vLLM health endpoint\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{endpoint_url}/health\", timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return \"\u2705 Healthy\", response.text\n",
        "        else:\n",
        "            return f\"\u274c Status: {response.status_code}\", response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return \"\u274c Connection Failed\", str(e)\n",
        "\n",
        "print(\"\ud83c\udfe5 Testing vLLM Health Endpoint\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "if endpoint not in [\"kubectl-not-available\"]:\n",
        "    status, response = test_vllm_health(endpoint)\n",
        "    print(f\"Health Status: {status}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    \n",
        "    if \"Healthy\" in status:\n",
        "        print(\"\\n\ud83c\udf89 vLLM service is healthy and ready for AI inference!\")\n",
        "    else:\n",
        "        print(\"\\n\u26a0\ufe0f Service may still be starting up. Wait a few minutes and retry.\")\nelse:\n",
        "    print(\"\u274c Cannot test health endpoint - service not accessible\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test vLLM API with a simple completion request\n",
        "def test_vllm_completion(endpoint_url, prompt, max_tokens=100):\n",
        "    \"\"\"Test vLLM completion endpoint\"\"\"\n",
        "    try:\n",
        "        payload = {\n",
        "            \"model\": \"microsoft/Llama-3.2-1B-Instruct\",\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "        \n",
        "        response = requests.post(\n",
        "            f\"{endpoint_url}/v1/completions\",\n",
        "            json=payload,\n",
        "            headers={\"Content-Type\": \"application/json\"},\n",
        "            timeout=30\n",
        "        )\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            return \"\u2705 Success\", response.json()\n",
        "        else:\n",
        "            return f\"\u274c Status: {response.status_code}\", response.text\n",
        "            \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return \"\u274c Request Failed\", str(e)\n",
        "\n",
        "print(\"\ud83e\udde0 Testing vLLM AI Completion\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "test_prompt = \"The benefits of using Kubernetes for AI workloads include:\"\n",
        "\n",
        "if endpoint not in [\"kubectl-not-available\"]:\n",
        "    print(f\"\ud83d\udcdd Prompt: {test_prompt}\")\n",
        "    print(\"\\n\ud83d\udd04 Generating AI response...\")\n",
        "    \n",
        "    status, response = test_vllm_completion(endpoint, test_prompt, max_tokens=150)\n",
        "    print(f\"\\nStatus: {status}\")\n",
        "    \n",
        "    if \"Success\" in status:\n",
        "        completion = response['choices'][0]['text']\n",
        "        print(f\"\\n\ud83e\udd16 AI Response: {completion}\")\n",
        "        print(f\"\\n\ud83d\udcca Usage Stats: {response.get('usage', 'N/A')}\")\n",
        "        print(\"\\n\ud83c\udf89 Congratulations! Your AI inference service is working perfectly!\")\n",
        "    else:\n",
        "        print(f\"Error: {response}\")\n",
        "        print(\"\\n\ud83d\udca1 The service may still be initializing. Wait a few minutes and retry.\")\nelse:\n",
        "    print(\"\u274c Cannot test AI completion - service not accessible\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcc8 Section 5: Interactive Scaling and Monitoring\n",
        "\n",
        "Let's explore Kubernetes' scaling capabilities with GPU workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate scaling the deployment\n",
        "print(\"\ud83d\ude80 Interactive Scaling Demonstration\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    # Check current scale\n",
        "    returncode, current_replicas, _ = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.spec.replicas}'\")\n",
        "    returncode2, ready_replicas, _ = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.status.readyReplicas}'\")\n",
        "    \n",
        "    if returncode == 0:\n",
        "        print(f\"\ud83d\udcca Current Scale:\")\n",
        "        print(f\"   Desired Replicas: {current_replicas}\")\n",
        "        print(f\"   Ready Replicas: {ready_replicas if returncode2 == 0 else 'Unknown'}\")\n",
        "        \n",
        "        # Scale to 2 replicas if currently 1, or to 1 if currently 2+\n",
        "        current_count = int(current_replicas) if current_replicas.isdigit() else 1\n",
        "        target_count = 2 if current_count == 1 else 1\n",
        "        \n",
        "        print(f\"\\n\ud83d\udcc8 Scaling to {target_count} replica(s)...\")\n",
        "        returncode, scale_result, error = run_kubectl(f\"scale deployment vllm-inference --replicas={target_count}\")\n",
        "        \n",
        "        if returncode == 0:\n",
        "            print(f\"\u2705 Scale command executed successfully\")\n",
        "            \n",
        "            # Wait and check new status\n",
        "            print(\"\\n\u23f3 Waiting for scaling to take effect...\")\n",
        "            time.sleep(15)\n",
        "            \n",
        "            returncode, new_status, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "            if returncode == 0:\n",
        "                print(f\"\\n\ud83d\udcca Updated Deployment Status:\")\n",
        "                print(new_status)\n",
        "            \n",
        "            # Show pods\n",
        "            returncode, pod_status, _ = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "            if returncode == 0:\n",
        "                print(\"\\n\ud83d\udce6 Pod Status:\")\n",
        "                print(pod_status)\n",
        "        else:\n",
        "            print(f\"\u274c Scale command failed: {error}\")\n",
        "    else:\n",
        "        print(\"\u274c vLLM deployment not found\")\nelse:\n",
        "    print(\"\u274c kubectl not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor GPU resource usage\n",
        "print(\"\ud83d\udcbe GPU Resource Monitoring\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    # Check GPU allocation across nodes\n",
        "    print(\"\ud83d\udda5\ufe0f GPU Resources per Node:\")\n",
        "    returncode, gpu_allocation, _ = run_kubectl('get nodes -o custom-columns=NAME:.metadata.name,\"TOTAL_GPU:.status.capacity.amd\\.com/gpu\",\"ALLOCATABLE_GPU:.status.allocatable.amd\\.com/gpu\"')\n",
        "    if returncode == 0:\n",
        "        print(gpu_allocation)\n",
        "    else:\n",
        "        print(\"\u274c Could not get GPU allocation info\")\n",
        "    \n",
        "    # Check which pods are using GPUs\n",
        "    print(\"\\n\ud83c\udfaf Current GPU Workloads:\")\n",
        "    returncode, gpu_pods, _ = run_kubectl('get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,NODE:.spec.nodeName,\"GPU_REQUEST:.spec.containers[*].resources.requests.amd\\.com/gpu\"')\n",
        "    if returncode == 0:\n",
        "        # Filter only pods that actually request GPUs\n",
        "        lines = gpu_pods.split('\\n')\n",
        "        header = lines[0]\n",
        "        gpu_requesting_pods = [line for line in lines[1:] if line and not line.endswith('<none>') and len(line.split()) >= 4 and line.split()[-1] not in ['<none>', '']]\n",
        "        \n",
        "        if gpu_requesting_pods:\n",
        "            print(header)\n",
        "            for pod in gpu_requesting_pods:\n",
        "                print(pod)\n",
        "        else:\n",
        "            print(\"   No pods currently requesting GPUs\")\n",
        "    else:\n",
        "        print(\"\u274c Could not check GPU usage by pods\")\n",
        "    \n",
        "    # Show cluster events (last 5)\n",
        "    print(\"\\n\ud83d\udccb Recent Cluster Events:\")\n",
        "    returncode, events, _ = run_kubectl(\"get events --sort-by=.metadata.creationTimestamp | tail -5\")\n",
        "    if returncode == 0:\n",
        "        print(events)\n",
        "    else:\n",
        "        print(\"\u274c Could not get cluster events\")\nelse:\n",
        "    print(\"\u274c kubectl not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Section 6: Interactive Troubleshooting Tools\n",
        "\n",
        "Essential commands and tools for managing GPU workloads in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive troubleshooting toolkit\n",
        "print(\"\ud83d\udd0d Interactive Troubleshooting Toolkit\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    print(\"\\n1\ufe0f\u20e3 Cluster Health Check:\")\n",
        "    returncode, nodes, _ = run_kubectl(\"get nodes\")\n",
        "    if returncode == 0:\n",
        "        print(nodes)\n",
        "    \n",
        "    print(\"\\n2\ufe0f\u20e3 System Pods Status:\")\n",
        "    returncode, system_pods, _ = run_kubectl(\"get pods -n kube-system | head -10\")\n",
        "    if returncode == 0:\n",
        "        print(system_pods)\n",
        "    \n",
        "    print(\"\\n3\ufe0f\u20e3 GPU Operator Status:\")\n",
        "    returncode, gpu_pods, _ = run_kubectl(\"get pods -n kube-amd-gpu\")\n",
        "    if returncode == 0:\n",
        "        print(gpu_pods)\n",
        "    else:\n",
        "        print(\"   GPU Operator not installed or pods not found\")\n",
        "    \n",
        "    print(\"\\n4\ufe0f\u20e3 vLLM Application Status:\")\n",
        "    returncode, vllm_pods, _ = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "    if returncode == 0:\n",
        "        print(vllm_pods)\n",
        "    else:\n",
        "        print(\"   vLLM not deployed or pods not found\")\n",
        "    \n",
        "    print(\"\\n5\ufe0f\u20e3 Service Status:\")\n",
        "    returncode, services, _ = run_kubectl(\"get services\")\n",
        "    if returncode == 0:\n",
        "        print(services)\n",
        "else:\n",
        "    print(\"\u274c kubectl not available - cannot run troubleshooting commands\")\n",
        "\n",
        "# Quick troubleshooting reference\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\ud83d\udee0\ufe0f Quick Troubleshooting Reference:\")\n",
        "print(\"\\nTo check specific component logs:\")\n",
        "print(\"\u2022 kubectl logs -l app=vllm-inference\")\n",
        "print(\"\u2022 kubectl logs -n kube-amd-gpu -l app.kubernetes.io/name=gpu-operator-charts\")\n",
        "print(\"\u2022 kubectl logs -n kube-system -l k8s-app=calico-node\")\n",
        "print(\"\\nTo describe resources:\")\n",
        "print(\"\u2022 kubectl describe node <node-name>\")\n",
        "print(\"\u2022 kubectl describe pod <pod-name>\")\n",
        "print(\"\u2022 kubectl describe deployment vllm-inference\")\n",
        "print(\"\\nTo restart failed pods:\")\n",
        "print(\"\u2022 kubectl delete pod <pod-name>  # Pod will be recreated\")\n",
        "print(\"\u2022 kubectl rollout restart deployment vllm-inference\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive cluster summary report\n",
        "print(\"\ud83d\udccb Comprehensive Cluster Summary Report\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "summary_data = {}\n",
        "\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    # Cluster nodes\n",
        "    returncode, nodes, _ = run_kubectl(\"get nodes --no-headers\")\n",
        "    if returncode == 0:\n",
        "        node_count = len([line for line in nodes.split('\\n') if line.strip()])\n",
        "        ready_nodes = len([line for line in nodes.split('\\n') if 'Ready' in line])\n",
        "        summary_data[\"Cluster Status\"] = f\"{ready_nodes}/{node_count} nodes ready\"\n",
        "    else:\n",
        "        summary_data[\"Cluster Status\"] = \"Not accessible\"\n",
        "    \n",
        "    # AMD GPU nodes\n",
        "    returncode, gpu_nodes, _ = run_kubectl(\"get nodes -l feature.node.kubernetes.io/amd-gpu=true --no-headers\")\n",
        "    if returncode == 0:\n",
        "        gpu_node_count = len([line for line in gpu_nodes.split('\\n') if line.strip()])\n",
        "        summary_data[\"AMD GPU Nodes\"] = str(gpu_node_count)\n",
        "    else:\n",
        "        summary_data[\"AMD GPU Nodes\"] = \"0\"\n",
        "    \n",
        "    # GPU Operator status\n",
        "    returncode, _, _ = run_kubectl(\"get namespace kube-amd-gpu\")\n",
        "    summary_data[\"GPU Operator\"] = \"\u2705 Installed\" if returncode == 0 else \"\u274c Not Installed\"\n",
        "    \n",
        "    # vLLM deployment\n",
        "    returncode, vllm_status, _ = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.status.readyReplicas}/{.spec.replicas}'\")\n",
        "    if returncode == 0 and vllm_status:\n",
        "        summary_data[\"vLLM Deployment\"] = f\"\u2705 Running ({vllm_status} replicas)\"\n",
        "    else:\n",
        "        summary_data[\"vLLM Deployment\"] = \"\u274c Not Deployed\"\n",
        "    \n",
        "    # Total GPU resources\n",
        "    returncode, gpu_capacity, _ = run_kubectl('get nodes -o jsonpath=\"{.items[*].status.capacity.amd\\.com/gpu}\"')\n",
        "    if returncode == 0 and gpu_capacity.strip():\n",
        "        try:\n",
        "            gpus = [int(x) for x in gpu_capacity.split() if x.isdigit()]\n",
        "            total_gpus = sum(gpus) if gpus else 0\n",
        "            summary_data[\"Total GPU Resources\"] = str(total_gpus)\n",
        "        except:\n",
        "            summary_data[\"Total GPU Resources\"] = \"0\"\n",
        "    else:\n",
        "        summary_data[\"Total GPU Resources\"] = \"0\"\n",
        "    \n",
        "    # Service accessibility\n",
        "    returncode, service_info, _ = run_kubectl(\"get service vllm-service\")\n",
        "    if returncode == 0:\n",
        "        if \"LoadBalancer\" in service_info:\n",
        "            summary_data[\"AI Service Access\"] = \"\u2705 LoadBalancer\"\n",
        "        else:\n",
        "            summary_data[\"AI Service Access\"] = \"\u2705 Available (NodePort)\"\n",
        "    else:\n",
        "        summary_data[\"AI Service Access\"] = \"\u274c Not Available\"\n",
        "else:\n",
        "    summary_data = {\n",
        "        \"Kubernetes\": \"\u274c Not Installed\",\n",
        "        \"Recommendation\": \"Execute Kubernetes installation cell above\"\n",
        "    }\n",
        "\n",
        "# Display summary\n",
        "for key, value in summary_data.items():\n",
        "    print(f\"\u2022 {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Determine overall status\n",
        "if check_command_exists(\"kubectl\") and \"\u2705 Installed\" in summary_data.get(\"GPU Operator\", \"\") and \"\u2705 Running\" in summary_data.get(\"vLLM Deployment\", \"\"):\n",
        "    print(\"\ud83c\udf89 CONGRATULATIONS! Your GPU-Accelerated AI Platform is Ready!\")\n",
        "    print(\"\\n\ud83d\ude80 What you've accomplished:\")\n",
        "    print(\"   \u2705 Kubernetes cluster with AMD GPU support\")\n",
        "    print(\"   \u2705 Production-ready AI inference service\")\n",
        "    print(\"   \u2705 Scalable, cloud-native architecture\")\n",
        "    print(\"   \u2705 Complete monitoring and troubleshooting toolkit\")\n",
        "    \n",
        "    print(\"\\n\ud83c\udfaf Next Steps for Production:\")\n",
        "    print(\"   \u2022 Deploy your own AI models\")\n",
        "    print(\"   \u2022 Set up Prometheus/Grafana monitoring\")\n",
        "    print(\"   \u2022 Implement autoscaling policies\")\n",
        "    print(\"   \u2022 Configure resource quotas for multi-tenancy\")\n",
        "    print(\"   \u2022 Explore multi-GPU model parallelism\")\nelse:\n",
        "    print(\"\ud83d\udd27 Setup Status: Some components need attention\")\n",
        "    print(\"\\n\ud83d\udca1 Next Steps:\")\n",
        "    if not check_command_exists(\"kubectl\"):\n",
        "        print(\"   \u2022 Execute the Kubernetes installation cell\")\n",
        "    if \"\u274c Not Installed\" in summary_data.get(\"GPU Operator\", \"\"):\n",
        "        print(\"   \u2022 Execute the AMD GPU Operator installation cell\")\n",
        "    if \"\u274c Not Deployed\" in summary_data.get(\"vLLM Deployment\", \"\"):\n",
        "        print(\"   \u2022 Execute the vLLM deployment cell\")\n",
        "    print(\"   \u2022 Re-run this summary cell to check progress\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf93 Tutorial Complete: Key Takeaways and Next Steps\n",
        "\n",
        "### \ud83c\udf89 What You've Accomplished\n",
        "\n",
        "Congratulations! You've successfully built a complete GPU-accelerated AI infrastructure stack:\n",
        "\n",
        "1. **\u2705 Complete Infrastructure Setup**: From bare Ubuntu server to production-ready GPU cluster\n",
        "2. **\u2705 AMD GPU Integration**: Seamlessly integrated MI300X GPUs with Kubernetes\n",
        "3. **\u2705 AI Inference Deployment**: Deployed production-ready vLLM service with load balancing\n",
        "4. **\u2705 Scaling & Monitoring**: Demonstrated horizontal scaling and resource monitoring\n",
        "5. **\u2705 Self-Service Experience**: Everything executed directly in Jupyter - no terminal needed!\n",
        "\n",
        "### \ud83c\udfd7\ufe0f Architecture You've Built\n",
        "\n",
        "```\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502              Jupyter Notebook               \u2502\n",
        "\u2502         (Interactive Management)            \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502            vLLM AI Service                  \u2502\n",
        "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n",
        "\u2502  \u2502 Load Balancer\u2502    \u2502  Auto Scaling\u2502       \u2502\n",
        "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502           Kubernetes Orchestration          \u2502\n",
        "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
        "\u2502  \u2502   Pods   \u2502 \u2502 Services \u2502 \u2502Deployments\u2502   \u2502\n",
        "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502           AMD GPU Operator                  \u2502\n",
        "\u2502           (Resource Management)             \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502         AMD Instinct MI300X GPUs            \u2502\n",
        "\u2502              (192GB HBM3)                   \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "### \ud83c\udfaf Production Considerations\n",
        "\n",
        "- **Security**: Implement RBAC, network policies, and pod security standards\n",
        "- **High Availability**: Deploy across multiple nodes with anti-affinity rules\n",
        "- **Monitoring**: Add Prometheus/Grafana for comprehensive observability\n",
        "- **Backup**: Implement backup strategies for persistent data and configurations\n",
        "- **Cost Optimization**: Use resource quotas, limits, and spot instances\n",
        "\n",
        "### \ud83d\udcda Learn More\n",
        "\n",
        "- **[AMD GPU Operator Documentation](https://rocm.github.io/gpu-operator/)**\n",
        "- **[vLLM Documentation](https://docs.vllm.ai/)**\n",
        "- **[Kubernetes GPU Scheduling](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)**\n",
        "- **[ROCm Blog Series](https://rocm.blogs.amd.com/artificial-intelligence/k8s-orchestration-part1/README.html)**\n",
        "\n",
        "### \ud83d\ude80 You're Ready for Production!\n",
        "\n",
        "Your infrastructure is now ready to handle enterprise AI workloads. You've mastered the complete stack from bare metal to production AI services - all through an interactive, self-contained Jupyter experience!\n",
        "\n",
        "**Happy AI inferencing! \ud83e\udd16\u2728**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}