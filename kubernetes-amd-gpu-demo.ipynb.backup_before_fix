{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with Kubernetes on AMD GPUs - Interactive Tutorial\n",
        "\n",
        "**Target Audience**: Infrastructure administrators and DevOps teams exploring AMD GPUs for production Kubernetes workloads\n",
        "\n",
        "This notebook provides hands-on experience with deploying and managing AI inference workloads on Kubernetes clusters with AMD GPUs.\n",
        "\n",
        "## Prerequisites\n",
        "- Ubuntu/Debian server with AMD GPUs\n",
        "- Root/sudo access for system-level operations\n",
        "- At least 2GB RAM and 20GB free disk space\n",
        "- Internet connectivity for package downloads\n",
        "\n",
        "## üéØ Tutorial Approach\n",
        "This notebook is **completely self-contained**. You can execute all installation and deployment steps directly within Jupyter cells - no need to switch to terminal!\n",
        "\n",
        "## üö® IMPORTANT: Getting Started\n",
        "**Before running any other cells:**\n",
        "1. **üìñ Read through the entire notebook first** to understand the process\n",
        "2. **‚ö° Run the \"Ensure Scripts Are Executable\" cell** (Step 0 below) to fix file permissions\n",
        "3. **üîç Run the \"Debugging\" cell** if you encounter any issues\n",
        "4. **üìã Follow the numbered sections in order**\n",
        "\n",
        "This tutorial will take you from a fresh system to a production-ready GPU-accelerated AI platform in about 30-45 minutes.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Section 0: System Prerequisites Check\n",
        "\n",
        "Let's start by checking system requirements and current status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import pandas as pd\n",
        "\n",
        "def run_command(command, check=False, show_output=True):\n",
        "    \"\"\"Helper function to run shell commands and return output\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            command, \n",
        "            shell=True, \n",
        "            capture_output=True, \n",
        "            text=True, \n",
        "            check=check\n",
        "        )\n",
        "        if show_output and result.stdout:\n",
        "            print(result.stdout)\n",
        "        if show_output and result.stderr and result.returncode != 0:\n",
        "            print(f\"Error: {result.stderr}\")\n",
        "        return result.returncode, result.stdout.strip(), result.stderr.strip()\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        if show_output:\n",
        "            print(f\"Command failed: {e}\")\n",
        "        return e.returncode, e.stdout.strip() if e.stdout else \"\", e.stderr.strip() if e.stderr else \"\"\n",
        "\n",
        "def run_kubectl(command):\n",
        "    \"\"\"Helper function to run kubectl commands and return output\"\"\"\n",
        "    return run_command(f\"kubectl {command}\", show_output=False)\n",
        "\n",
        "def check_command_exists(command):\n",
        "    \"\"\"Check if a command exists in the system\"\"\"\n",
        "    returncode, _, _ = run_command(f\"which {command}\", show_output=False)\n",
        "    return returncode == 0\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded\")\n",
        "print(\"üéØ Ready to execute installation scripts directly in the notebook!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Step 0: Ensure Scripts Are Executable\n",
        "\n",
        "**üö® IMPORTANT: Run this cell first!**\n",
        "\n",
        "This cell ensures all installation scripts have the correct executable permissions. File uploads to cloud platforms often lose executable permissions, so we fix that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import stat\n",
        "import subprocess\n",
        "\n",
        "# Helper functions for the tutorial\n",
        "def run_bash_command(command, capture_output=False):\n",
        "    \"\"\"Execute bash command and show output\"\"\"\n",
        "    try:\n",
        "        if capture_output:\n",
        "            result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "            return result.stdout.strip() if result.returncode == 0 else None\n",
        "        else:\n",
        "            result = subprocess.run(command, shell=True, text=True)\n",
        "            return result.returncode == 0\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error executing command: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_command_exists(command):\n",
        "    \"\"\"Check if a command exists in the system\"\"\"\n",
        "    return subprocess.run(f\"command -v {command}\", shell=True, capture_output=True).returncode == 0\n",
        "\n",
        "print(\"--- Step 0: Ensure ALL Scripts Are Executable ---\")\n",
        "print(\"üîß Making sure all tutorial scripts have proper permissions...\")\n",
        "\n",
        "# Complete list of all scripts in this tutorial\n",
        "all_scripts = [\n",
        "    \"install-kubernetes.sh\",\n",
        "    \"install-kubernetes-container.sh\",\n",
        "    \"install-amd-gpu-operator.sh\",\n",
        "    \"deploy-vllm-inference.sh\",\n",
        "    \"detect-environment.sh\",\n",
        "    \"verify-readiness.sh\",\n",
        "]\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "print(f\"üìÅ Current working directory: {current_dir}\")\n",
        "print(f\"üìã Checking {len(all_scripts)} scripts...\")\n",
        "\n",
        "missing_scripts = []\n",
        "fixed_scripts = []\n",
        "already_executable = []\n",
        "\n",
        "for script in all_scripts:\n",
        "    script_path = os.path.join(current_dir, script)\n",
        "    if os.path.exists(script_path):\n",
        "        current_permissions = stat.S_IMODE(os.stat(script_path).st_mode)\n",
        "        is_executable = os.access(script_path, os.X_OK)\n",
        "        \n",
        "        if not is_executable:\n",
        "            print(f\"üîß Making {script} executable...\")\n",
        "            os.chmod(script_path, 0o755)\n",
        "            new_permissions = stat.S_IMODE(os.stat(script_path).st_mode)\n",
        "            is_executable = os.access(script_path, os.X_OK)\n",
        "            if is_executable:\n",
        "                fixed_scripts.append(script)\n",
        "                print(f\"   ‚úÖ Fixed: {script} (now {oct(new_permissions)})\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå Failed to fix: {script}\")\n",
        "        else:\n",
        "            already_executable.append(script)\n",
        "            print(f\"   ‚úÖ Already executable: {script} ({oct(current_permissions)})\")\n",
        "    else:\n",
        "        missing_scripts.append(script)\n",
        "        print(f\"   ‚ö†Ô∏è  Missing: {script}\")\n",
        "\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"   ‚úÖ Already executable: {len(already_executable)} scripts\")\n",
        "print(f\"   üîß Fixed permissions: {len(fixed_scripts)} scripts\")\n",
        "print(f\"   ‚ö†Ô∏è  Missing scripts: {len(missing_scripts)} scripts\")\n",
        "\n",
        "if missing_scripts:\n",
        "    print(f\"\\n‚ö†Ô∏è  Missing scripts:\")\n",
        "    for script in missing_scripts:\n",
        "        print(f\"      ‚Ä¢ {script}\")\n",
        "\n",
        "if fixed_scripts:\n",
        "    print(f\"\\nüîß Fixed scripts:\")\n",
        "    for script in fixed_scripts:\n",
        "        print(f\"      ‚Ä¢ {script}\")\n",
        "\n",
        "print(f\"\\nüéØ All available scripts are now executable and ready for use!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Check current working directory and script availability\n",
        "print(\"üîç Debugging Script Location and Permissions\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "import os\n",
        "\n",
        "# Check current working directory\n",
        "current_dir = os.getcwd()\n",
        "print(f\"üìÅ Current working directory: {current_dir}\")\n",
        "\n",
        "# List files in current directory\n",
        "print(f\"\\nüìã Files in current directory:\")\n",
        "try:\n",
        "    files = os.listdir('.')\n",
        "    for file in sorted(files):\n",
        "        if os.path.isfile(file):\n",
        "            size = os.path.getsize(file)\n",
        "            permissions = oct(os.stat(file).st_mode)[-3:]\n",
        "            executable = \"‚úÖ\" if os.access(file, os.X_OK) else \"‚ùå\"\n",
        "            print(f\"   {file} ({size} bytes, {permissions}, executable: {executable})\")\n",
        "        elif os.path.isdir(file):\n",
        "            print(f\"   üìÅ {file}/\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error listing files: {e}\")\n",
        "\n",
        "# Specifically check for our scripts\n",
        "scripts_to_check = [\n",
        "    'install-kubernetes.sh',\n",
        "    'install-amd-gpu-operator.sh', \n",
        "    'deploy-vllm-inference.sh'\n",
        "]\n",
        "\n",
        "print(f\"\\nüîç Checking for required scripts:\")\n",
        "for script in scripts_to_check:\n",
        "    if os.path.exists(script):\n",
        "        is_executable = os.access(script, os.X_OK)\n",
        "        size = os.path.getsize(script)\n",
        "        print(f\"   ‚úÖ {script} (exists, {size} bytes, executable: {'‚úÖ' if is_executable else '‚ùå'})\")\n",
        "        \n",
        "        # Check if we can read the first few lines\n",
        "        try:\n",
        "            with open(script, 'r') as f:\n",
        "                first_line = f.readline().strip()\n",
        "                print(f\"      First line: {first_line}\")\n",
        "        except:\n",
        "            print(f\"      ‚ö†Ô∏è Cannot read file\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {script} (not found)\")\n",
        "\n",
        "# Test command execution from notebook\n",
        "print(f\"\\nüß™ Testing command execution:\")\n",
        "returncode, output, error = run_command(\"pwd\", show_output=False)\n",
        "print(f\"   pwd command: {output}\")\n",
        "\n",
        "returncode, output, error = run_command(\"whoami\", show_output=False)\n",
        "print(f\"   Current user: {output}\")\n",
        "\n",
        "returncode, output, error = run_command(\"ls -la *.sh 2>/dev/null || echo 'No .sh files found'\", show_output=False)\n",
        "print(f\"   Shell scripts found: {output}\")\n",
        "\n",
        "# Check if we can execute scripts with full path\n",
        "print(f\"\\nüîß Testing script execution:\")\n",
        "full_path_script = os.path.join(current_dir, 'install-kubernetes.sh')\n",
        "returncode, output, error = run_command(f\"ls -la {full_path_script}\", show_output=False)\n",
        "print(f\"   Full path check: {output if output else error}\")\n",
        "\n",
        "if os.path.exists('install-kubernetes.sh'):\n",
        "    # Test if we can execute with full path\n",
        "    returncode, output, error = run_command(f\"{full_path_script} --help 2>/dev/null || echo 'Script cannot be executed'\", show_output=False)\n",
        "    print(f\"   Script execution test: {output[:100] if output else error[:100]}\")\n",
        "\n",
        "print(f\"\\nüí° Troubleshooting tips:\")\n",
        "print(f\"   ‚Ä¢ If scripts are not executable: run 'chmod +x *.sh' in a terminal\")\n",
        "print(f\"   ‚Ä¢ If scripts are missing: ensure you're in the correct directory\")\n",
        "print(f\"   ‚Ä¢ Current directory should contain all .sh scripts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîß Fix Script Permissions (Run if needed)\n",
        "\n",
        "If the debug cell above shows that scripts are not executable or not found, run this cell to fix permissions and verify location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix script permissions and verify location\n",
        "print(\"üîß Fixing Script Permissions and Location\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "import os\n",
        "\n",
        "# Change to the correct directory (same as notebook)\n",
        "notebook_dir = os.path.dirname(os.path.abspath('__file__')) if '__file__' in globals() else os.getcwd()\n",
        "print(f\"üìÅ Ensuring we're in notebook directory: {notebook_dir}\")\n",
        "os.chdir(notebook_dir)\n",
        "\n",
        "# Make scripts executable\n",
        "scripts = ['install-kubernetes.sh', 'install-amd-gpu-operator.sh', 'deploy-vllm-inference.sh']\n",
        "\n",
        "print(f\"\\nüîß Making scripts executable:\")\n",
        "for script in scripts:\n",
        "    if os.path.exists(script):\n",
        "        try:\n",
        "            # Make executable\n",
        "            os.chmod(script, 0o755)\n",
        "            print(f\"   ‚úÖ {script} - permissions fixed\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå {script} - failed to fix permissions: {e}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {script} - file not found\")\n",
        "\n",
        "# Verify the fix worked\n",
        "print(f\"\\n‚úÖ Verification:\")\n",
        "returncode, output, error = run_command(\"ls -la *.sh\", show_output=False)\n",
        "if returncode == 0:\n",
        "    print(f\"   Script permissions:\")\n",
        "    for line in output.split('\\n'):\n",
        "        if line.strip():\n",
        "            print(f\"     {line}\")\n",
        "else:\n",
        "    print(f\"   ‚ùå Could not list scripts: {error}\")\n",
        "\n",
        "# Test execution\n",
        "if os.path.exists('install-kubernetes.sh'):\n",
        "    print(f\"\\nüß™ Testing script execution:\")\n",
        "    returncode, output, error = run_command(\"./install-kubernetes.sh --help 2>&1 | head -3\", show_output=False)\n",
        "    if 'command not found' not in output.lower() and 'permission denied' not in output.lower():\n",
        "        print(f\"   ‚úÖ Script can be executed\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Script execution issue: {output[:100]}\")\n",
        "\n",
        "print(f\"\\nüí° If issues persist, try running the following in a terminal:\")\n",
        "print(f\"   cd {os.getcwd()}\")\n",
        "print(f\"   chmod +x *.sh\")\n",
        "print(f\"   ls -la *.sh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prerequisites check\n",
        "print(\"üîç System Prerequisites Check\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check OS\n",
        "returncode, os_info, _ = run_command(\"cat /etc/os-release | grep PRETTY_NAME\", show_output=False)\n",
        "if returncode == 0:\n",
        "    print(f\"üìü Operating System: {os_info.split('=')[1].strip('\\\"')}\")\n",
        "else:\n",
        "    print(\"‚ùå Could not detect OS\")\n",
        "\n",
        "# Check memory\n",
        "returncode, memory_info, _ = run_command(\"free -h | grep Mem\", show_output=False)\n",
        "if returncode == 0:\n",
        "    memory_total = memory_info.split()[1]\n",
        "    print(f\"üíæ Total Memory: {memory_total}\")\n",
        "    \n",
        "    # Extract numeric value for comparison\n",
        "    memory_gb = float(memory_total.replace('Gi', '').replace('G', '').replace('Mi', '').replace('M', ''))\n",
        "    if 'Mi' in memory_total or 'M' in memory_total:\n",
        "        memory_gb = memory_gb / 1024\n",
        "    \n",
        "    if memory_gb < 2:\n",
        "        print(\"‚ö†Ô∏è Warning: Less than 2GB RAM detected. Kubernetes may not perform well.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Memory check passed\")\n",
        "\n",
        "# Check disk space\n",
        "returncode, disk_info, _ = run_command(\"df -h / | tail -1\", show_output=False)\n",
        "if returncode == 0:\n",
        "    disk_available = disk_info.split()[3]\n",
        "    print(f\"üíø Available Disk Space: {disk_available}\")\n",
        "\n",
        "# Check for AMD GPUs\n",
        "returncode, gpu_info, _ = run_command(\"lspci | grep -i amd\", show_output=False)\n",
        "if returncode == 0 and gpu_info:\n",
        "    print(\"‚úÖ AMD GPUs detected:\")\n",
        "    for line in gpu_info.split('\\n'):\n",
        "        if line.strip():\n",
        "            print(f\"   üéÆ {line.strip()}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No AMD GPUs detected\")\n",
        "\n",
        "# Check sudo access\n",
        "returncode, _, _ = run_command(\"sudo -n true\", show_output=False)\n",
        "if returncode == 0:\n",
        "    print(\"‚úÖ Root/sudo access available\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Root/sudo access may be required for some operations\")\n",
        "\n",
        "print(\"\\n‚úÖ System prerequisites check completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Kubernetes installation status\n",
        "print(\"üîç Kubernetes Installation Status\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check if Kubernetes components are installed\n",
        "kubectl_installed = check_command_exists(\"kubectl\")\n",
        "kubeadm_installed = check_command_exists(\"kubeadm\")\n",
        "kubelet_installed = check_command_exists(\"kubelet\")\n",
        "\n",
        "print(f\"kubectl installed: {'‚úÖ' if kubectl_installed else '‚ùå'}\")\n",
        "print(f\"kubeadm installed: {'‚úÖ' if kubeadm_installed else '‚ùå'}\")\n",
        "print(f\"kubelet installed: {'‚úÖ' if kubelet_installed else '‚ùå'}\")\n",
        "\n",
        "# If kubectl is installed, check cluster connectivity\n",
        "cluster_accessible = False\n",
        "if kubectl_installed:\n",
        "    returncode, version_output, error = run_kubectl(\"version --client\")\n",
        "    if returncode == 0:\n",
        "        print(f\"\\nüìã kubectl version: {version_output.split()[2] if len(version_output.split()) > 2 else 'Unknown'}\")\n",
        "        \n",
        "        # Check cluster access\n",
        "        returncode, cluster_info, error = run_kubectl(\"cluster-info\")\n",
        "        if returncode == 0:\n",
        "            print(\"‚úÖ Kubernetes cluster is accessible\")\n",
        "            cluster_accessible = True\n",
        "            \n",
        "            # Show basic cluster info\n",
        "            print(\"\\nüìä Cluster Information:\")\n",
        "            for line in cluster_info.split('\\n')[:3]:  # First 3 lines\n",
        "                if line.strip():\n",
        "                    print(f\"   {line.strip()}\")\n",
        "        else:\n",
        "            print(\"‚ùå Kubernetes cluster not accessible\")\n",
        "            print(f\"   Error: {error}\")\n",
        "\n",
        "# Store status for next cells\n",
        "kubernetes_needs_installation = not kubectl_installed or not kubeadm_installed or not kubelet_installed\n",
        "kubernetes_needs_cluster_setup = kubectl_installed and not cluster_accessible\n",
        "\n",
        "# Determine what needs to be done\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "if kubernetes_needs_installation:\n",
        "    print(\"üö® KUBERNETES INSTALLATION REQUIRED\")\n",
        "    print(\"\\nüí° Execute the next cell to install Kubernetes automatically!\")\n",
        "elif kubernetes_needs_cluster_setup:\n",
        "    print(\"üö® KUBERNETES CLUSTER SETUP REQUIRED\")\n",
        "    print(\"\\nüí° Execute the next cell to initialize the Kubernetes cluster!\")\n",
        "else:\n",
        "    print(\"‚úÖ KUBERNETES IS READY\")\n",
        "    print(\"\\nüéØ You can skip to the AMD GPU Operator installation section.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Section 1: One-Click Kubernetes Installation\n",
        "\n",
        "**Execute this cell only if Kubernetes installation is required** (as indicated by the check above).\n",
        "\n",
        "This cell will execute the complete Kubernetes installation script directly in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Smart Environment Detection\n",
        "\n",
        "This tutorial automatically detects your environment and selects the appropriate installation method:\n",
        "\n",
        "### üñ•Ô∏è Host Environment\n",
        "- **Detected when**: Running on bare metal or VM with systemd\n",
        "- **Uses**: `install-kubernetes.sh`\n",
        "- **Installs**: Full single-node Kubernetes cluster\n",
        "- **Result**: Production-ready cluster with AMD GPU support\n",
        "\n",
        "### üê≥ Container Environment\n",
        "- **Detected when**: Running inside Docker/container or without systemd\n",
        "- **Uses**: `install-kubernetes-container.sh`\n",
        "- **Installs**: kubectl tools + kind (Kubernetes in Docker)\n",
        "- **Result**: Development cluster suitable for testing\n",
        "\n",
        "The notebook automatically chooses the right approach for your environment! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"--- Phase 0: Install Vanilla Kubernetes ---\")\n",
        "print(\"This step will install Kubernetes components appropriate for your environment.\")\n",
        "print(\"Detecting environment and selecting the right installation method...\")\n",
        "\n",
        "# Detect if running in container environment\n",
        "def detect_container_environment():\n",
        "    \"\"\"Detect if running in a container environment\"\"\"\n",
        "    try:\n",
        "        container_indicators = [\n",
        "            os.path.exists(\"/.dockerenv\"),\n",
        "            \"docker\" in open(\"/proc/1/cgroup\", \"r\").read() if os.path.exists(\"/proc/1/cgroup\") else False,\n",
        "            \"container\" in os.environ.get(\"container\", \"\"),\n",
        "        ]\n",
        "        return any(container_indicators)\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Detect if systemd is available\n",
        "def detect_systemd():\n",
        "    \"\"\"Check if systemd is available and running as PID 1\"\"\"\n",
        "    try:\n",
        "        with open(\"/proc/1/comm\", \"r\") as f:\n",
        "            init_process = f.read().strip()\n",
        "        return init_process == \"systemd\"\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Environment detection\n",
        "is_container = detect_container_environment()\n",
        "has_systemd = detect_systemd()\n",
        "\n",
        "print(f\"\\nüîç Environment Detection:\")\n",
        "print(f\"   ‚Ä¢ Container Environment: {'‚úÖ Yes' if is_container else '‚ùå No'}\")\n",
        "print(f\"   ‚Ä¢ Systemd Available: {'‚úÖ Yes' if has_systemd else '‚ùå No'}\")\n",
        "\n",
        "# Select appropriate script\n",
        "if is_container or not has_systemd:\n",
        "    script_name = \"install-kubernetes-container.sh\"\n",
        "    print(f\"\\nüê≥ Container environment detected - using {script_name}\")\n",
        "    print(\"This will install kubectl tools and optionally create a kind cluster.\")\n",
        "else:\n",
        "    script_name = \"install-kubernetes.sh\"\n",
        "    print(f\"\\nüñ•Ô∏è  Host environment detected - using {script_name}\")\n",
        "    print(\"This will install a full single-node Kubernetes cluster.\")\n",
        "\n",
        "# Ensure the script is executable\n",
        "script_path = os.path.join(os.getcwd(), script_name)\n",
        "if os.path.exists(script_path):\n",
        "    if not os.access(script_path, os.X_OK):\n",
        "        print(f\"üîß Making {script_path} executable...\")\n",
        "        os.chmod(script_path, 0o755)\n",
        "    print(f\"‚úÖ Using script: {script_name}\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: {script_path} not found.\")\n",
        "    print(\"\\nAvailable scripts:\")\n",
        "    for script in [\"install-kubernetes.sh\", \"install-kubernetes-container.sh\"]:\n",
        "        if os.path.exists(script):\n",
        "            print(f\"  ‚úÖ {script}\")\n",
        "        else:\n",
        "            print(f\"  ‚ùå {script}\")\n",
        "    raise FileNotFoundError(f\"Script {script_path} not found.\")\n",
        "\n",
        "print(f\"\\nüöÄ Starting installation with {script_name}...\")\n",
        "print(\"This may take 10-15 minutes. Please monitor the output for any errors.\")\n",
        "\n",
        "# Execute the appropriate script\n",
        "run_bash_command(f\"sudo {script_path}\")\n",
        "\n",
        "print(f\"\\n--- Kubernetes Installation with {script_name} Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üõ†Ô∏è Common Installation Issues and Solutions\n",
        "\n",
        "If the Kubernetes installation encounters issues, here are common problems and solutions:\n",
        "\n",
        "#### üîß Kernel Module Issues\n",
        "**Error**: `modprobe: FATAL: Module overlay not found`\n",
        "- **Cause**: Some cloud instances don't have overlay kernel modules\n",
        "- **Solution**: The script automatically handles this with fallbacks\n",
        "- **Status**: ‚úÖ Already fixed in the installation script\n",
        "\n",
        "#### üíæ Memory Issues\n",
        "**Error**: Installation fails due to insufficient memory\n",
        "- **Cause**: Less than 2GB RAM available\n",
        "- **Solution**: Use a larger instance or add swap space\n",
        "\n",
        "#### üåê Network Issues\n",
        "**Error**: Cannot download packages\n",
        "- **Cause**: Internet connectivity or firewall issues\n",
        "- **Solution**: Check internet access and security group settings\n",
        "\n",
        "#### üîê Permission Issues\n",
        "**Error**: Permission denied during installation\n",
        "- **Cause**: Insufficient sudo privileges\n",
        "- **Solution**: Ensure you have sudo access or run as root\n",
        "\n",
        "If issues persist, check the error messages above and refer to the troubleshooting section at the end of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÆ Section 2: One-Click AMD GPU Operator Installation\n",
        "\n",
        "Now let's install the AMD GPU Operator to enable GPU support in our Kubernetes cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if AMD GPU Operator is already installed\n",
        "print(\"üéØ AMD GPU Operator Installation Check\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "gpu_operator_installed = False\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    returncode, _, _ = run_kubectl(\"get namespace kube-amd-gpu\")\n",
        "    if returncode == 0:\n",
        "        print(\"‚úÖ AMD GPU Operator namespace found\")\n",
        "        \n",
        "        # Check GPU operator pods\n",
        "        returncode, gpu_pods, _ = run_kubectl(\"get pods -n kube-amd-gpu\")\n",
        "        if returncode == 0:\n",
        "            print(\"\\nüîß GPU Operator Pods:\")\n",
        "            print(gpu_pods)\n",
        "            gpu_operator_installed = True\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è GPU Operator namespace exists but pods not found\")\n",
        "    else:\n",
        "        print(\"‚ùå AMD GPU Operator not installed\")\nelse:\n",
        "    print(\"‚ùå kubectl not available - install Kubernetes first\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if gpu_operator_installed:\n",
        "    print(\"‚úÖ AMD GPU OPERATOR IS ALREADY INSTALLED\")\n",
        "    print(\"\\nüéØ You can skip to the vLLM deployment section.\")\nelse:\n",
        "    print(\"üö® AMD GPU OPERATOR INSTALLATION REQUIRED\")\n",
        "    print(\"\\nüí° Execute the next cell to install AMD GPU Operator!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-click AMD GPU Operator installation with improved error handling\n",
        "print(\"üéÆ AMD GPU Operator Installation\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if not gpu_operator_installed:\n",
        "    print(\"üì¶ Starting AMD GPU Operator installation...\")\n",
        "    print(\"‚è±Ô∏è This will take 5-10 minutes.\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    # Ensure script exists and is executable\n",
        "    import os\n",
        "    script_path = os.path.join(os.getcwd(), 'install-amd-gpu-operator.sh')\n",
        "    \n",
        "    if not os.path.exists(script_path):\n",
        "        print(f\"‚ùå Script not found at: {script_path}\")\n",
        "        print(\"üí° Please run the debug/fix cells above.\")\n",
        "    else:\n",
        "        os.chmod(script_path, 0o755)\n",
        "        \n",
        "        # Execute the AMD GPU Operator installation script\n",
        "        print(f\"üîß Executing: {script_path}\")\n",
        "        returncode, output, error = run_command(script_path, show_output=True)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        if returncode == 0:\n",
        "            print(\"‚úÖ AMD GPU Operator installation completed successfully!\")\n",
        "            \n",
        "            # Check GPU resources\n",
        "            print(\"\\nüîÑ Checking GPU resources...\")\n",
        "            time.sleep(10)\n",
        "            returncode, gpu_resources, _ = run_kubectl('get nodes -o custom-columns=NAME:.metadata.name,\"Total GPUs:.status.capacity.amd\\.com/gpu\",\"Allocatable GPUs:.status.allocatable.amd\\.com/gpu\"')\n",
        "            if returncode == 0:\n",
        "                print(\"\\nüíæ GPU Resources:\")\n",
        "                print(gpu_resources)\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è GPU resources not yet visible. They may take a few minutes to appear.\")\n",
        "        else:\n",
        "            print(f\"‚ùå AMD GPU Operator installation failed with return code: {returncode}\")\n",
        "            print(\"\\nüí° Check the error messages above and retry if needed.\")\nelse:\n",
        "    print(\"‚úÖ AMD GPU Operator is already installed.\")\n",
        "    print(\"\\nüéØ Proceeding to next section...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Section 3: One-Click vLLM AI Inference Deployment\n",
        "\n",
        "Now let's deploy a production-ready AI inference service using vLLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if vLLM is already deployed\n",
        "print(\"ü§ñ vLLM Deployment Status Check\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "vllm_deployed = False\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    returncode, _, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "    if returncode == 0:\n",
        "        print(\"‚úÖ vLLM deployment found\")\n",
        "        \n",
        "        # Check deployment status\n",
        "        returncode, deployment_status, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "        if returncode == 0:\n",
        "            print(\"\\nüìä Deployment Status:\")\n",
        "            print(deployment_status)\n",
        "            vllm_deployed = True\n",
        "        \n",
        "        # Check service\n",
        "        returncode, service_status, _ = run_kubectl(\"get service vllm-service\")\n",
        "        if returncode == 0:\n",
        "            print(\"\\nüåê Service Status:\")\n",
        "            print(service_status)\n",
        "    else:\n",
        "        print(\"‚ùå vLLM deployment not found\")\nelse:\n",
        "    print(\"‚ùå kubectl not available\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if vllm_deployed:\n",
        "    print(\"‚úÖ vLLM IS ALREADY DEPLOYED\")\n",
        "    print(\"\\nüéØ You can proceed to testing the API.\")\nelse:\n",
        "    print(\"üö® vLLM DEPLOYMENT REQUIRED\")\n",
        "    print(\"\\nüí° Execute the next cell to deploy vLLM!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-click vLLM deployment with improved error handling\n",
        "print(\"ü§ñ vLLM AI Inference Deployment\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "if not vllm_deployed:\n",
        "    print(\"üì¶ Starting vLLM deployment...\")\n",
        "    print(\"‚è±Ô∏è This will take 5-10 minutes (includes downloading the model).\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    # Ensure script exists and is executable\n",
        "    import os\n",
        "    script_path = os.path.join(os.getcwd(), 'deploy-vllm-inference.sh')\n",
        "    \n",
        "    if not os.path.exists(script_path):\n",
        "        print(f\"‚ùå Script not found at: {script_path}\")\n",
        "        print(\"üí° Please run the debug/fix cells above.\")\n",
        "    else:\n",
        "        os.chmod(script_path, 0o755)\n",
        "        \n",
        "        # Execute the vLLM deployment script\n",
        "        print(f\"üîß Executing: {script_path}\")\n",
        "        returncode, output, error = run_command(script_path, show_output=True)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        if returncode == 0:\n",
        "            print(\"‚úÖ vLLM deployment completed successfully!\")\n",
        "            \n",
        "            # Check deployment status\n",
        "            print(\"\\nüîÑ Checking deployment status...\")\n",
        "            time.sleep(5)\n",
        "            \n",
        "            returncode, pods, _ = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "            if returncode == 0:\n",
        "                print(\"\\nüì¶ vLLM Pods:\")\n",
        "                print(pods)\n",
        "            \n",
        "            returncode, service, _ = run_kubectl(\"get service vllm-service\")\n",
        "            if returncode == 0:\n",
        "                print(\"\\nüåê vLLM Service:\")\n",
        "                print(service)\n",
        "        else:\n",
        "            print(f\"‚ùå vLLM deployment failed with return code: {returncode}\")\n",
        "            print(\"\\nüí° Check the error messages above and retry if needed.\")\nelse:\n",
        "    print(\"‚úÖ vLLM is already deployed.\")\n",
        "    print(\"\\nüéØ Proceeding to API testing...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Section 4: Test AI Inference API\n",
        "\n",
        "Let's test our deployed AI inference service!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get service endpoint for API testing\n",
        "def get_vllm_endpoint():\n",
        "    \"\"\"Get the vLLM service endpoint\"\"\"\n",
        "    if not check_command_exists(\"kubectl\"):\n",
        "        return \"kubectl-not-available\"\n",
        "        \n",
        "    try:\n",
        "        # Try to get LoadBalancer external IP\n",
        "        returncode, external_ip, _ = run_kubectl(\"get service vllm-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\")\n",
        "        if returncode == 0 and external_ip and external_ip != \"null\" and external_ip.strip():\n",
        "            return f\"http://{external_ip.strip()}\"\n",
        "        \n",
        "        # Fallback to NodePort\n",
        "        returncode, node_ip, _ = run_kubectl(\"get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\")\n",
        "        returncode2, node_port, _ = run_kubectl(\"get service vllm-service -o jsonpath='{.spec.ports[0].nodePort}'\")\n",
        "        \n",
        "        if returncode == 0 and returncode2 == 0 and node_ip and node_port:\n",
        "            return f\"http://{node_ip.strip()}:{node_port.strip()}\"\n",
        "        \n",
        "        # Last resort: port-forward indication\n",
        "        return \"port-forward\"\n",
        "    except:\n",
        "        return \"port-forward\"\n",
        "\n",
        "print(\"üåç vLLM Service Endpoint Detection\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "endpoint = get_vllm_endpoint()\n",
        "\n",
        "if endpoint == \"kubectl-not-available\":\n",
        "    print(\"‚ùå kubectl not available - cannot detect vLLM endpoint\")\nelif endpoint == \"port-forward\":\n",
        "    print(\"‚ö†Ô∏è External access not available. Setting up port-forward...\")\n",
        "    print(\"\\nüîß Creating port-forward tunnel...\")\n",
        "    \n",
        "    # Start port-forward in background\n",
        "    import threading\n",
        "    import subprocess\n",
        "    \n",
        "    def port_forward():\n",
        "        subprocess.run([\"kubectl\", \"port-forward\", \"service/vllm-service\", \"8000:8000\"])\n",
        "    \n",
        "    # Start port-forward in a separate thread\n",
        "    pf_thread = threading.Thread(target=port_forward, daemon=True)\n",
        "    pf_thread.start()\n",
        "    time.sleep(5)  # Give port-forward time to establish\n",
        "    \n",
        "    endpoint = \"http://localhost:8000\"\n",
        "    print(f\"‚úÖ Port-forward established. Using: {endpoint}\")\nelse:\n",
        "    print(f\"‚úÖ vLLM Service accessible at: {endpoint}\")\n",
        "    print(f\"   API endpoint: {endpoint}/v1/completions\")\n",
        "    print(f\"   Health check: {endpoint}/health\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test vLLM API health endpoint\n",
        "def test_vllm_health(endpoint_url):\n",
        "    \"\"\"Test vLLM health endpoint\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{endpoint_url}/health\", timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return \"‚úÖ Healthy\", response.text\n",
        "        else:\n",
        "            return f\"‚ùå Status: {response.status_code}\", response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return \"‚ùå Connection Failed\", str(e)\n",
        "\n",
        "print(\"üè• Testing vLLM Health Endpoint\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "if endpoint not in [\"kubectl-not-available\"]:\n",
        "    status, response = test_vllm_health(endpoint)\n",
        "    print(f\"Health Status: {status}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    \n",
        "    if \"Healthy\" in status:\n",
        "        print(\"\\nüéâ vLLM service is healthy and ready for AI inference!\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Service may still be starting up. Wait a few minutes and retry.\")\nelse:\n",
        "    print(\"‚ùå Cannot test health endpoint - service not accessible\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test vLLM API with a simple completion request\n",
        "def test_vllm_completion(endpoint_url, prompt, max_tokens=100):\n",
        "    \"\"\"Test vLLM completion endpoint\"\"\"\n",
        "    try:\n",
        "        payload = {\n",
        "            \"model\": \"microsoft/Llama-3.2-1B-Instruct\",\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "        \n",
        "        response = requests.post(\n",
        "            f\"{endpoint_url}/v1/completions\",\n",
        "            json=payload,\n",
        "            headers={\"Content-Type\": \"application/json\"},\n",
        "            timeout=30\n",
        "        )\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            return \"‚úÖ Success\", response.json()\n",
        "        else:\n",
        "            return f\"‚ùå Status: {response.status_code}\", response.text\n",
        "            \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return \"‚ùå Request Failed\", str(e)\n",
        "\n",
        "print(\"üß† Testing vLLM AI Completion\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "test_prompt = \"The benefits of using Kubernetes for AI workloads include:\"\n",
        "\n",
        "if endpoint not in [\"kubectl-not-available\"]:\n",
        "    print(f\"üìù Prompt: {test_prompt}\")\n",
        "    print(\"\\nüîÑ Generating AI response...\")\n",
        "    \n",
        "    status, response = test_vllm_completion(endpoint, test_prompt, max_tokens=150)\n",
        "    print(f\"\\nStatus: {status}\")\n",
        "    \n",
        "    if \"Success\" in status:\n",
        "        completion = response['choices'][0]['text']\n",
        "        print(f\"\\nü§ñ AI Response: {completion}\")\n",
        "        print(f\"\\nüìä Usage Stats: {response.get('usage', 'N/A')}\")\n",
        "        print(\"\\nüéâ Congratulations! Your AI inference service is working perfectly!\")\n",
        "    else:\n",
        "        print(f\"Error: {response}\")\n",
        "        print(\"\\nüí° The service may still be initializing. Wait a few minutes and retry.\")\nelse:\n",
        "    print(\"‚ùå Cannot test AI completion - service not accessible\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Section 5: Interactive Scaling and Monitoring\n",
        "\n",
        "Let's explore Kubernetes' scaling capabilities with GPU workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate scaling the deployment\n",
        "print(\"üöÄ Interactive Scaling Demonstration\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    # Check current scale\n",
        "    returncode, current_replicas, _ = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.spec.replicas}'\")\n",
        "    returncode2, ready_replicas, _ = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.status.readyReplicas}'\")\n",
        "    \n",
        "    if returncode == 0:\n",
        "        print(f\"üìä Current Scale:\")\n",
        "        print(f\"   Desired Replicas: {current_replicas}\")\n",
        "        print(f\"   Ready Replicas: {ready_replicas if returncode2 == 0 else 'Unknown'}\")\n",
        "        \n",
        "        # Scale to 2 replicas if currently 1, or to 1 if currently 2+\n",
        "        current_count = int(current_replicas) if current_replicas.isdigit() else 1\n",
        "        target_count = 2 if current_count == 1 else 1\n",
        "        \n",
        "        print(f\"\\nüìà Scaling to {target_count} replica(s)...\")\n",
        "        returncode, scale_result, error = run_kubectl(f\"scale deployment vllm-inference --replicas={target_count}\")\n",
        "        \n",
        "        if returncode == 0:\n",
        "            print(f\"‚úÖ Scale command executed successfully\")\n",
        "            \n",
        "            # Wait and check new status\n",
        "            print(\"\\n‚è≥ Waiting for scaling to take effect...\")\n",
        "            time.sleep(15)\n",
        "            \n",
        "            returncode, new_status, _ = run_kubectl(\"get deployment vllm-inference\")\n",
        "            if returncode == 0:\n",
        "                print(f\"\\nüìä Updated Deployment Status:\")\n",
        "                print(new_status)\n",
        "            \n",
        "            # Show pods\n",
        "            returncode, pod_status, _ = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "            if returncode == 0:\n",
        "                print(\"\\nüì¶ Pod Status:\")\n",
        "                print(pod_status)\n",
        "        else:\n",
        "            print(f\"‚ùå Scale command failed: {error}\")\n",
        "    else:\n",
        "        print(\"‚ùå vLLM deployment not found\")\nelse:\n",
        "    print(\"‚ùå kubectl not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor GPU resource usage\n",
        "print(\"üíæ GPU Resource Monitoring\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    # Check GPU allocation across nodes\n",
        "    print(\"üñ•Ô∏è GPU Resources per Node:\")\n",
        "    returncode, gpu_allocation, _ = run_kubectl('get nodes -o custom-columns=NAME:.metadata.name,\"TOTAL_GPU:.status.capacity.amd\\.com/gpu\",\"ALLOCATABLE_GPU:.status.allocatable.amd\\.com/gpu\"')\n",
        "    if returncode == 0:\n",
        "        print(gpu_allocation)\n",
        "    else:\n",
        "        print(\"‚ùå Could not get GPU allocation info\")\n",
        "    \n",
        "    # Check which pods are using GPUs\n",
        "    print(\"\\nüéØ Current GPU Workloads:\")\n",
        "    returncode, gpu_pods, _ = run_kubectl('get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,NODE:.spec.nodeName,\"GPU_REQUEST:.spec.containers[*].resources.requests.amd\\.com/gpu\"')\n",
        "    if returncode == 0:\n",
        "        # Filter only pods that actually request GPUs\n",
        "        lines = gpu_pods.split('\\n')\n",
        "        header = lines[0]\n",
        "        gpu_requesting_pods = [line for line in lines[1:] if line and not line.endswith('<none>') and len(line.split()) >= 4 and line.split()[-1] not in ['<none>', '']]\n",
        "        \n",
        "        if gpu_requesting_pods:\n",
        "            print(header)\n",
        "            for pod in gpu_requesting_pods:\n",
        "                print(pod)\n",
        "        else:\n",
        "            print(\"   No pods currently requesting GPUs\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not check GPU usage by pods\")\n",
        "    \n",
        "    # Show cluster events (last 5)\n",
        "    print(\"\\nüìã Recent Cluster Events:\")\n",
        "    returncode, events, _ = run_kubectl(\"get events --sort-by=.metadata.creationTimestamp | tail -5\")\n",
        "    if returncode == 0:\n",
        "        print(events)\n",
        "    else:\n",
        "        print(\"‚ùå Could not get cluster events\")\nelse:\n",
        "    print(\"‚ùå kubectl not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Section 6: Interactive Troubleshooting Tools\n",
        "\n",
        "Essential commands and tools for managing GPU workloads in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive troubleshooting toolkit\n",
        "print(\"üîç Interactive Troubleshooting Toolkit\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    print(\"\\n1Ô∏è‚É£ Cluster Health Check:\")\n",
        "    returncode, nodes, _ = run_kubectl(\"get nodes\")\n",
        "    if returncode == 0:\n",
        "        print(nodes)\n",
        "    \n",
        "    print(\"\\n2Ô∏è‚É£ System Pods Status:\")\n",
        "    returncode, system_pods, _ = run_kubectl(\"get pods -n kube-system | head -10\")\n",
        "    if returncode == 0:\n",
        "        print(system_pods)\n",
        "    \n",
        "    print(\"\\n3Ô∏è‚É£ GPU Operator Status:\")\n",
        "    returncode, gpu_pods, _ = run_kubectl(\"get pods -n kube-amd-gpu\")\n",
        "    if returncode == 0:\n",
        "        print(gpu_pods)\n",
        "    else:\n",
        "        print(\"   GPU Operator not installed or pods not found\")\n",
        "    \n",
        "    print(\"\\n4Ô∏è‚É£ vLLM Application Status:\")\n",
        "    returncode, vllm_pods, _ = run_kubectl(\"get pods -l app=vllm-inference\")\n",
        "    if returncode == 0:\n",
        "        print(vllm_pods)\n",
        "    else:\n",
        "        print(\"   vLLM not deployed or pods not found\")\n",
        "    \n",
        "    print(\"\\n5Ô∏è‚É£ Service Status:\")\n",
        "    returncode, services, _ = run_kubectl(\"get services\")\n",
        "    if returncode == 0:\n",
        "        print(services)\n",
        "else:\n",
        "    print(\"‚ùå kubectl not available - cannot run troubleshooting commands\")\n",
        "\n",
        "# Quick troubleshooting reference\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üõ†Ô∏è Quick Troubleshooting Reference:\")\n",
        "print(\"\\nTo check specific component logs:\")\n",
        "print(\"‚Ä¢ kubectl logs -l app=vllm-inference\")\n",
        "print(\"‚Ä¢ kubectl logs -n kube-amd-gpu -l app.kubernetes.io/name=gpu-operator-charts\")\n",
        "print(\"‚Ä¢ kubectl logs -n kube-system -l k8s-app=calico-node\")\n",
        "print(\"\\nTo describe resources:\")\n",
        "print(\"‚Ä¢ kubectl describe node <node-name>\")\n",
        "print(\"‚Ä¢ kubectl describe pod <pod-name>\")\n",
        "print(\"‚Ä¢ kubectl describe deployment vllm-inference\")\n",
        "print(\"\\nTo restart failed pods:\")\n",
        "print(\"‚Ä¢ kubectl delete pod <pod-name>  # Pod will be recreated\")\n",
        "print(\"‚Ä¢ kubectl rollout restart deployment vllm-inference\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive cluster summary report\n",
        "print(\"üìã Comprehensive Cluster Summary Report\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "summary_data = {}\n",
        "\n",
        "if check_command_exists(\"kubectl\"):\n",
        "    # Cluster nodes\n",
        "    returncode, nodes, _ = run_kubectl(\"get nodes --no-headers\")\n",
        "    if returncode == 0:\n",
        "        node_count = len([line for line in nodes.split('\\n') if line.strip()])\n",
        "        ready_nodes = len([line for line in nodes.split('\\n') if 'Ready' in line])\n",
        "        summary_data[\"Cluster Status\"] = f\"{ready_nodes}/{node_count} nodes ready\"\n",
        "    else:\n",
        "        summary_data[\"Cluster Status\"] = \"Not accessible\"\n",
        "    \n",
        "    # AMD GPU nodes\n",
        "    returncode, gpu_nodes, _ = run_kubectl(\"get nodes -l feature.node.kubernetes.io/amd-gpu=true --no-headers\")\n",
        "    if returncode == 0:\n",
        "        gpu_node_count = len([line for line in gpu_nodes.split('\\n') if line.strip()])\n",
        "        summary_data[\"AMD GPU Nodes\"] = str(gpu_node_count)\n",
        "    else:\n",
        "        summary_data[\"AMD GPU Nodes\"] = \"0\"\n",
        "    \n",
        "    # GPU Operator status\n",
        "    returncode, _, _ = run_kubectl(\"get namespace kube-amd-gpu\")\n",
        "    summary_data[\"GPU Operator\"] = \"‚úÖ Installed\" if returncode == 0 else \"‚ùå Not Installed\"\n",
        "    \n",
        "    # vLLM deployment\n",
        "    returncode, vllm_status, _ = run_kubectl(\"get deployment vllm-inference -o jsonpath='{.status.readyReplicas}/{.spec.replicas}'\")\n",
        "    if returncode == 0 and vllm_status:\n",
        "        summary_data[\"vLLM Deployment\"] = f\"‚úÖ Running ({vllm_status} replicas)\"\n",
        "    else:\n",
        "        summary_data[\"vLLM Deployment\"] = \"‚ùå Not Deployed\"\n",
        "    \n",
        "    # Total GPU resources\n",
        "    returncode, gpu_capacity, _ = run_kubectl('get nodes -o jsonpath=\"{.items[*].status.capacity.amd\\.com/gpu}\"')\n",
        "    if returncode == 0 and gpu_capacity.strip():\n",
        "        try:\n",
        "            gpus = [int(x) for x in gpu_capacity.split() if x.isdigit()]\n",
        "            total_gpus = sum(gpus) if gpus else 0\n",
        "            summary_data[\"Total GPU Resources\"] = str(total_gpus)\n",
        "        except:\n",
        "            summary_data[\"Total GPU Resources\"] = \"0\"\n",
        "    else:\n",
        "        summary_data[\"Total GPU Resources\"] = \"0\"\n",
        "    \n",
        "    # Service accessibility\n",
        "    returncode, service_info, _ = run_kubectl(\"get service vllm-service\")\n",
        "    if returncode == 0:\n",
        "        if \"LoadBalancer\" in service_info:\n",
        "            summary_data[\"AI Service Access\"] = \"‚úÖ LoadBalancer\"\n",
        "        else:\n",
        "            summary_data[\"AI Service Access\"] = \"‚úÖ Available (NodePort)\"\n",
        "    else:\n",
        "        summary_data[\"AI Service Access\"] = \"‚ùå Not Available\"\n",
        "else:\n",
        "    summary_data = {\n",
        "        \"Kubernetes\": \"‚ùå Not Installed\",\n",
        "        \"Recommendation\": \"Execute Kubernetes installation cell above\"\n",
        "    }\n",
        "\n",
        "# Display summary\n",
        "for key, value in summary_data.items():\n",
        "    print(f\"‚Ä¢ {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Determine overall status\n",
        "if check_command_exists(\"kubectl\") and \"‚úÖ Installed\" in summary_data.get(\"GPU Operator\", \"\") and \"‚úÖ Running\" in summary_data.get(\"vLLM Deployment\", \"\"):\n",
        "    print(\"üéâ CONGRATULATIONS! Your GPU-Accelerated AI Platform is Ready!\")\n",
        "    print(\"\\nüöÄ What you've accomplished:\")\n",
        "    print(\"   ‚úÖ Kubernetes cluster with AMD GPU support\")\n",
        "    print(\"   ‚úÖ Production-ready AI inference service\")\n",
        "    print(\"   ‚úÖ Scalable, cloud-native architecture\")\n",
        "    print(\"   ‚úÖ Complete monitoring and troubleshooting toolkit\")\n",
        "    \n",
        "    print(\"\\nüéØ Next Steps for Production:\")\n",
        "    print(\"   ‚Ä¢ Deploy your own AI models\")\n",
        "    print(\"   ‚Ä¢ Set up Prometheus/Grafana monitoring\")\n",
        "    print(\"   ‚Ä¢ Implement autoscaling policies\")\n",
        "    print(\"   ‚Ä¢ Configure resource quotas for multi-tenancy\")\n",
        "    print(\"   ‚Ä¢ Explore multi-GPU model parallelism\")\nelse:\n",
        "    print(\"üîß Setup Status: Some components need attention\")\n",
        "    print(\"\\nüí° Next Steps:\")\n",
        "    if not check_command_exists(\"kubectl\"):\n",
        "        print(\"   ‚Ä¢ Execute the Kubernetes installation cell\")\n",
        "    if \"‚ùå Not Installed\" in summary_data.get(\"GPU Operator\", \"\"):\n",
        "        print(\"   ‚Ä¢ Execute the AMD GPU Operator installation cell\")\n",
        "    if \"‚ùå Not Deployed\" in summary_data.get(\"vLLM Deployment\", \"\"):\n",
        "        print(\"   ‚Ä¢ Execute the vLLM deployment cell\")\n",
        "    print(\"   ‚Ä¢ Re-run this summary cell to check progress\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Tutorial Complete: Key Takeaways and Next Steps\n",
        "\n",
        "### üéâ What You've Accomplished\n",
        "\n",
        "Congratulations! You've successfully built a complete GPU-accelerated AI infrastructure stack:\n",
        "\n",
        "1. **‚úÖ Complete Infrastructure Setup**: From bare Ubuntu server to production-ready GPU cluster\n",
        "2. **‚úÖ AMD GPU Integration**: Seamlessly integrated MI300X GPUs with Kubernetes\n",
        "3. **‚úÖ AI Inference Deployment**: Deployed production-ready vLLM service with load balancing\n",
        "4. **‚úÖ Scaling & Monitoring**: Demonstrated horizontal scaling and resource monitoring\n",
        "5. **‚úÖ Self-Service Experience**: Everything executed directly in Jupyter - no terminal needed!\n",
        "\n",
        "### üèóÔ∏è Architecture You've Built\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ              Jupyter Notebook               ‚îÇ\n",
        "‚îÇ         (Interactive Management)            ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ            vLLM AI Service                  ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ\n",
        "‚îÇ  ‚îÇ Load Balancer‚îÇ    ‚îÇ  Auto Scaling‚îÇ       ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ           Kubernetes Orchestration          ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
        "‚îÇ  ‚îÇ   Pods   ‚îÇ ‚îÇ Services ‚îÇ ‚îÇDeployments‚îÇ   ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ           AMD GPU Operator                  ‚îÇ\n",
        "‚îÇ           (Resource Management)             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ         AMD Instinct MI300X GPUs            ‚îÇ\n",
        "‚îÇ              (192GB HBM3)                   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### üéØ Production Considerations\n",
        "\n",
        "- **Security**: Implement RBAC, network policies, and pod security standards\n",
        "- **High Availability**: Deploy across multiple nodes with anti-affinity rules\n",
        "- **Monitoring**: Add Prometheus/Grafana for comprehensive observability\n",
        "- **Backup**: Implement backup strategies for persistent data and configurations\n",
        "- **Cost Optimization**: Use resource quotas, limits, and spot instances\n",
        "\n",
        "### üìö Learn More\n",
        "\n",
        "- **[AMD GPU Operator Documentation](https://rocm.github.io/gpu-operator/)**\n",
        "- **[vLLM Documentation](https://docs.vllm.ai/)**\n",
        "- **[Kubernetes GPU Scheduling](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)**\n",
        "- **[ROCm Blog Series](https://rocm.blogs.amd.com/artificial-intelligence/k8s-orchestration-part1/README.html)**\n",
        "\n",
        "### üöÄ You're Ready for Production!\n",
        "\n",
        "Your infrastructure is now ready to handle enterprise AI workloads. You've mastered the complete stack from bare metal to production AI services - all through an interactive, self-contained Jupyter experience!\n",
        "\n",
        "**Happy AI inferencing! ü§ñ‚ú®**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}